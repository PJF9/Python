{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NhVaa2BOnXlJ"
      },
      "outputs": [],
      "source": [
        "from Tokenizers import WordPiece\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import Accuracy, Recall, Precision, F1Score\n",
        "from torchinfo import summary\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from math import ceil\n",
        "from random import randint"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KG0bSY-inXlO"
      },
      "source": [
        "## Setting Device Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyEplS8SnXlR",
        "outputId": "7e4c1c1e-a590-481a-a0b4-8cf7e9a754f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X3inl2TSnXlS"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3oeozOiWnXlT",
        "outputId": "75224d86-3005-4c25-9b38-e8343f13d660"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ff8336ba-3205-4a01-8cb4-6d23958f8020\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff8336ba-3205-4a01-8cb4-6d23958f8020')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff8336ba-3205-4a01-8cb4-6d23958f8020 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff8336ba-3205-4a01-8cb4-6d23958f8020');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Label                                               Text\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro...\n",
              "...     ...                                                ...\n",
              "5567      1  This is the 2nd time we have tried 2 contact u...\n",
              "5568      0              Will Ì_ b going to esplanade fr home?\n",
              "5569      0  Pity, * was in mood for that. So...any other s...\n",
              "5570      0  The guy did some bitching but I acted like i'd...\n",
              "5571      0                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"sms_spam.csv\", encoding=\"latin-1\")[[\"v1\", \"v2\"]]\n",
        "df.rename(columns={\"v1\": \"Label\", \"v2\": \"Text\"}, inplace=True)\n",
        "\n",
        "df[\"Label\"] = df[\"Label\"].map({\n",
        "    \"ham\": 0,\n",
        "    \"spam\": 1\n",
        "})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX76lwoXnXlU",
        "outputId": "27e026de-2480-4dae-b394-f72d3e1ccc9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    4825\n",
              "1     747\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"Label\"].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K_I9QVOZnXlU"
      },
      "source": [
        "## Converting Dataset to Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MA9vAS9InXlV"
      },
      "outputs": [],
      "source": [
        "df[\"Cl_Text\"] = df[\"Text\"].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vc4tBQT5nXlV",
        "outputId": "4827b331-65e9-4c72-b4a1-0b7f23a38db9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-85449fc8-fa30-4cae-85b2-0289304e9ea5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cl_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>will ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>pity, * was in mood for that. so...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>the guy did some bitching but i acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>rofl. its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85449fc8-fa30-4cae-85b2-0289304e9ea5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85449fc8-fa30-4cae-85b2-0289304e9ea5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85449fc8-fa30-4cae-85b2-0289304e9ea5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Label                                               Text  \\\n",
              "0         0  Go until jurong point, crazy.. Available only ...   \n",
              "1         0                      Ok lar... Joking wif u oni...   \n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3         0  U dun say so early hor... U c already then say...   \n",
              "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
              "...     ...                                                ...   \n",
              "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
              "5568      0              Will Ì_ b going to esplanade fr home?   \n",
              "5569      0  Pity, * was in mood for that. So...any other s...   \n",
              "5570      0  The guy did some bitching but I acted like i'd...   \n",
              "5571      0                         Rofl. Its true to its name   \n",
              "\n",
              "                                                Cl_Text  \n",
              "0     go until jurong point, crazy.. available only ...  \n",
              "1                         ok lar... joking wif u oni...  \n",
              "2     free entry in 2 a wkly comp to win fa cup fina...  \n",
              "3     u dun say so early hor... u c already then say...  \n",
              "4     nah i don't think he goes to usf, he lives aro...  \n",
              "...                                                 ...  \n",
              "5567  this is the 2nd time we have tried 2 contact u...  \n",
              "5568              will ì_ b going to esplanade fr home?  \n",
              "5569  pity, * was in mood for that. so...any other s...  \n",
              "5570  the guy did some bitching but i acted like i'd...  \n",
              "5571                         rofl. its true to its name  \n",
              "\n",
              "[5572 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WPr0BBoXnXlW"
      },
      "source": [
        "## Converting Dataset into List of Strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLJQWlQFnXlX",
        "outputId": "f32dbc38-6209-47cf-da21-2815c592b2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5572\n",
            "i'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? i've cried enough today.\n"
          ]
        }
      ],
      "source": [
        "corpus = list(df.Cl_Text)\n",
        "\n",
        "print(len(corpus))\n",
        "print(corpus[10])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ym1PvFhnXlY"
      },
      "source": [
        "## Creating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-so2NVTDnXlZ"
      },
      "outputs": [],
      "source": [
        "w = WordPiece(corpus=corpus, ntokens=5_000, cleaning=lambda text: text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "388RdaJMnXlZ",
        "outputId": "aa76ff70-fca3-4003-cb81-831dd83110ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating Vocabulary: 100%|██████████| 4847/4847 [04:21<00:00, 18.50it/s]\n"
          ]
        }
      ],
      "source": [
        "w.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYKv4gTLnXla",
        "outputId": "883c0410-500f-435f-b190-ec65b8aa5437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'[CLS]': 0, '[UNK]': 1, '[PAD]': 2, '[SEP]': 3, '!': 4, '!!': 5, '!!!': 6, '!!!!': 7, \"!!'\": 8, \"!!''\": 9, \"!!''.\": 10, '!1': 11, '!:-)': 12, '#': 13, '##!': 14, '##!!': 15, '##!!!': 16, '##!!!!': 17, '##!!!!!!!!': 18, '##!!!!!!!!!': 19, '##!!!;-)': 20, '##!!:)': 21, '##!!;-)': 22, '##!!\\\\\"': 23, '##!\"': 24, '##!)xx': 25, '##!4': 26, '##!;-)': 27, '##!\\\\\"\"': 28, '##!t&cs': 29, '##!xxxx': 30, '##!\\x8eö´\\x89ó_': 31, '##!\\x8eö´\\x89ó_?': 32, '##!\\x8eö´\\x89ó_??': 33, '##!\\x8eö´\\x89ó_??\\x8bû¬': 34, '##!\\x8eö´\\x89ó_??\\x8bû¬u': 35, '##!\\x8eö´\\x89ó_??\\x8bû¬ud': 36, '##\"': 37, '###': 38, '###&': 39, '##$': 40, '##$7': 41, '##$70': 42, '##$700': 43, '##%': 44, '##&': 45, '##&100': 46, '##&amp;': 47, '##&c': 48, \"##&c'\": 49, \"##&c's\": 50, '##&c:': 51, '##&cs': 52, '##&csbcm4235wc1': 53, '##&csbcm4235wc1n3xx': 54, '##&csbcm4235wc1n3xx.': 55, '##&csbcm4235wc1n3xx.c': 56, '##&csbcm4235wc1n3xx.ca': 57, '##&csbcm4235wc1n3xx.cal': 58, '##&csbcm4235wc1n3xx.call': 59, '##&csbcm4235wc1n3xx.callc': 60, '##&g': 61, '##&gt': 62, '##&gt;': 63, \"##'\": 64, '##(': 65, '##(2': 66, '##(21': 67, '##(21/': 68, '##(21/f': 69, '##(21/f)': 70, '##(4': 71, '##(4m': 72, '##(4ms': 73, '##(4msg': 74, '##(4msgs': 75, '##(4msgs)': 76, '##(å£': 77, '##)': 78, \"##)'\": 79, '##)10': 80, '##)10,': 81, '##)xx': 82, '##*': 83, '##*\"': 84, '##**': 85, '##****': 86, '##******': 87, '##**********': 88, '##**************': 89, '##****7': 90, '##*378': 91, '##*å£1': 92, '##+': 93, '##+2': 94, '##+6*å£1': 95, '##+6+': 96, '##+?': 97, '##+b': 98, '##+g': 99, '##+m': 100, '##+å£1': 101, '##,': 102, '##,&amp;': 103, '##,000': 104, '##,22,65,61,66': 105, '##,400': 106, '##,50/': 107, '##,500': 108, '##,61,66': 109, '##,65,61,66': 110, '##,66': 111, '##-': 112, '##-$': 113, '##-$9': 114, '##-$90': 115, '##-$900': 116, '##-(': 117, '##-)': 118, '##--': 119, '##-/@': 120, '##-/@d': 121, '##-/@dr': 122, '##-/@drivby-:0qui': 123, '##-/@drivby-:0quit': 124, '##-05': 125, '##-08': 126, '##-0871872787': 127, '##-08718727870': 128, '##-087187278701': 129, '##-0871872787015': 130, '##-08718727870150': 131, '##-08718727870150p': 132, '##-08718727870150pp': 133, '##-08718727870150ppm': 134, '##-11': 135, '##-11p': 136, '##-165': 137, '##-2': 138, '##-2-': 139, '##-2-s': 140, '##-2-sh': 141, '##-4-1': 142, '##-434': 143, '##-440': 144, '##-5': 145, '##-6': 146, '##-7': 147, '##-7/': 148, '##-7/12': 149, '##-76': 150, '##-762': 151, '##-7p': 152, '##-8': 153, '##-82050': 154, '##-82050.': 155, '##-82050.c': 156, '##-8:30': 157, '##-9': 158, '##-91': 159, '##-910': 160, '##-9p': 161, '##-:0q': 162, '##-q': 163, '##-|': 164, '##-å£': 165, '##-å£5': 166, '##.': 167, '##.!!!;-)': 168, '##.!!:)': 169, '##.!!\\\\\"': 170, '##.!\\x8eö´\\x89ó_??\\x8bû¬ud': 171, '##.$700': 172, '##..!!!;-)': 173, '##..!!:)': 174, '##..!!\\\\\"': 175, '##..!\\x8eö´\\x89ó_??\\x8bû¬ud': 176, '##..$700': 177, '##...!!:)': 178, '##...!!\\\\\"': 179, '##...!\\x8eö´\\x89ó_??\\x8bû¬ud': 180, '##...$700': 181, '##....!!:)': 182, '##....!!\\\\\"': 183, '##....|': 184, '##...|': 185, '##...ìä': 186, '##..:)\"': 187, '##..:-);-)': 188, '##..:-d;-),': 189, '##..|': 190, '##..ìä': 191, '##.00': 192, '##.03': 193, '##.07781482378': 194, '##.100p': 195, '##.15': 196, '##.150pp': 197, '##.20': 198, '##.220cm2': 199, '##.23f': 200, '##.23g': 201, '##.30': 202, '##.3lp.msg@150p': 203, '##.4-': 204, '##.45pm': 205, '##.47p': 206, '##.48': 207, '##.49/m': 208, '##.50': 209, '##.50/m': 210, '##.50/w': 211, '##.5p/m': 212, '##.6p': 213, '##.80488': 214, '##.:)\"': 215, '##.:-);-)': 216, '##.:-d;-),': 217, '##.\\\\\":-)\"': 218, '##.b4u': 219, '##.g.23f': 220, '##.g.23g': 221, '##.msg@150p': 222, '##.sg/~phyhcmk/': 223, '##.subs16+1w': 224, '##.tscs087147403231winawk!ag': 225, '##.txt-2-sh': 226, '##.txt43.c': 227, '##.txt82228.c': 228, '##.win-82050.c': 229, '##.|': 230, '##.å£1,50/m': 231, '##.ì¬': 232, '##.ìä': 233, '##/': 234, '##//': 235, '##/04': 236, '##/06': 237, '##/06/': 238, '##/08': 239, '##/09/02': 240, '##/1': 241, '##/10/04': 242, '##/11/04': 243, '##/120': 244, '##/120p': 245, '##/150': 246, '##/150p': 247, '##/150p16+': 248, '##/2': 249, '##/200': 250, '##/200p': 251, '##/3': 252, '##/4/04': 253, '##/4q': 254, '##/4qf': 255, '##/4qf2': 256, '##/5': 257, '##/674': 258, '##/674&': 259, '##/7': 260, '##/9/03': 261, '##/@': 262, '##/d3wv': 263, '##/f4q=': 264, '##/icmb3ck': 265, '##/j': 266, '##/lf56': 267, '##/pc1323': 268, '##/q': 269, '##/w/icmb3ck': 270, '##/w1j': 271, '##/w1j6': 272, '##/~': 273, '##/~p': 274, '##/ì¼': 275, '##/ì¼1': 276, '##/ì¼1.': 277, '##/ì¼1.20': 278, '##0': 279, '##0%': 280, '##0-å£5': 281, '##0/06/': 282, '##00': 283, '##00%': 284, '##000': 285, '##000327': 286, '##0009307': 287, '##003': 288, '##00327': 289, '##004': 290, '##005': 291, '##006': 292, '##0062': 293, '##007': 294, '##0086': 295, '##009307': 296, '##01216+': 297, '##01327': 298, '##01327b': 299, '##0155': 300, '##027': 301, '##03': 302, '##030': 303, '##0303': 304, '##0327': 305, '##0353': 306, '##03530': 307, '##03530150': 308, '##03530150p': 309, '##03530150pm': 310, '##039': 311, '##04': 312, '##04/09/02': 313, '##0407165': 314, '##041': 315, '##0430': 316, '##0430-': 317, '##0430-j': 318, '##049': 319, '##05': 320, '##050000327': 321, '##0506': 322, '##053': 323, '##0533': 324, '##0554': 325, '##05572711&': 326, '##06': 327, '##062': 328, '##062117': 329, '##066': 330, '##069': 331, '##07': 332, '##077': 333, '##077632': 334, '##07781482378': 335, '##08': 336, '##0819': 337, '##08394': 338, '##086': 339, '##0870': 340, '##08700': 341, '##087006': 342, '##0870062': 343, '##08700621': 344, '##087006211': 345, '##0870062117': 346, '##08700621170': 347, '##087006211701': 348, '##0870062117015': 349, '##08700621170150': 350, '##08704': 351, '##087124': 352, '##0871474': 353, '##08714740': 354, '##087147403': 355, '##0871474032': 356, '##08714740323': 357, '##087147403231': 358, '##087147403231w': 359, '##08714742': 360, '##087147428': 361, '##0871474280': 362, '##08714742804': 363, '##0871872': 364, '##0871872787': 365, '##08718728876': 366, '##08726822': 367, '##0878': 368, '##089(': 369, '##09/02': 370, '##09050000327': 371, '##09307': 372, '##0938767': 373, '##0q': 374, '##1': 375, '##1(å£': 376, '##1,000': 377, '##1,66': 378, '##1.50': 379, '##1.50/w': 380, '##1.50p': 381, '##1.50pp': 382, '##1.50ppm': 383, '##1/04': 384, '##1/10/04': 385, '##1/11/04': 386, '##10': 387, '##10/04': 388, '##100': 389, '##100p': 390, '##100p/': 391, '##1010': 392, '##1018': 393, '##10183': 394, '##10303': 395, '##11': 396, '##11!': 397, '##11(å£': 398, '##11/04': 399, '##111': 400, '##111w': 401, '##111wx': 402, '##114': 403, '##114/': 404, '##114/14': 405, '##1146': 406, '##1151': 407, '##117': 408, '##12': 409, '##120': 410, '##1200': 411, '##1216+': 412, '##123': 413, '##128': 414, '##1282': 415, '##13': 416, '##1303': 417, '##1323': 418, '##1327': 419, '##139': 420, '##139w': 421, '##139wa': 422, '##14': 423, '##1403': 424, '##141701216+': 425, '##146': 426, '##15': 427, '##150': 428, '##150p': 429, '##150p/': 430, '##150pp': 431, '##150ppm': 432, '##150ppmm': 433, '##150ppmx3': 434, '##150x3+': 435, '##150x3+n': 436, '##151': 437, '##1510': 438, '##155': 439, '##156': 440, '##15h': 441, '##15hb': 442, '##16': 443, '##16+': 444, '##16+1': 445, '##16+1w': 446, '##16+å£1': 447, '##16+å£1.50': 448, '##16+å£1.50p': 449, '##16.150pp': 450, '##165': 451, '##168': 452, '##1685': 453, '##17': 454, '##1701216+': 455, '##1705572711&': 456, '##1705572711&f': 457, '##1705572711&fi': 458, '##1705572711&fir': 459, '##1705572711&firs': 460, '##172': 461, '##17678': 462, '##176781': 463, '##177': 464, '##1782': 465, '##1784': 466, '##18': 467, \"##18'\": 468, \"##18's\": 469, '##18+': 470, '##18+)': 471, '##1870': 472, '##1896': 473, '##1896w': 474, '##1896wc': 475, '##1896wc1': 476, '##18:0430-j': 477, '##18:0430-ju': 478, '##18:0430-jul': 479, '##18:0430-jul-05': 480, '##19': 481, '##195038': 482, '##199': 483, '##1a7rw18': 484, '##1j': 485, '##1j6': 486, '##1jy': 487, '##1n3xx': 488, '##1t1jy': 489, '##1win150ppmx3': 490, '##2': 491, '##2,000': 492, '##2,65,61,66': 493, '##20': 494, '##20-': 495, '##200': 496, '##2000': 497, '##2004': 498, '##202': 499, '##2049': 500, '##205': 501, '##2050': 502, '##20554': 503, '##21': 504, '##2117': 505, '##2150': 506, '##2150p': 507, '##2156': 508, '##216+': 509, '##217': 510, '##22': 511, '##22,65,61,66': 512, '##220': 513, '##220c': 514, '##220cm': 515, '##220cm2': 516, '##221': 517, '##221b': 518, '##221bp': 519, '##2220': 520, '##2228>>': 521, '##2242': 522, '##225': 523, '##226': 524, '##227': 525, '##2277': 526, '##227x': 527, '##228>>': 528, '##23': 529, '##23,': 530, '##230': 531, '##2323': 532, '##235': 533, '##239': 534, '##23f': 535, '##23g': 536, '##242': 537, '##245': 538, '##2468': 539, '##2478': 540, '##25': 541, '##250': 542, '##250.': 543, '##250.c': 544, '##26': 545, '##263': 546, '##27': 547, '##270': 548, '##2708': 549, '##27081': 550, '##270819': 551, '##2708198': 552, '##27081980': 553, '##2735=å£45': 554, '##277': 555, '##278': 556, '##278b': 557, '##278bb': 558, '##27y': 559, '##27yf': 560, '##28': 561, '##280114': 562, '##2807': 563, '##2810': 564, '##28>>': 565, '##29': 566, '##29/': 567, '##29869': 568, '##29c': 569, '##29c,': 570, '##2:': 571, '##2a.3lp.msg@150p': 572, '##3': 573, '##3+': 574, '##3,': 575, '##3/10/04': 576, '##3/4/04': 577, '##30': 578, '##3000': 579, '##303': 580, '##3039': 581, '##3049': 582, '##309': 583, '##31': 584, '##32': 585, '##32156': 586, '##323': 587, '##326': 588, '##327': 589, '##32w': 590, '##32wu': 591, '##32wu.': 592, '##334': 593, '##3355': 594, '##34': 595, '##341': 596, '##341.': 597, '##342': 598, '##342/': 599, '##342/2': 600, '##342/2l': 601, '##342/2la': 602, '##342/2lan': 603, '##342/2land': 604, '##342/2lands': 605, '##342/2lands/': 606, '##342/2lands/r': 607, '##345': 608, '##35': 609, '##350': 610, '##353': 611, '##355': 612, '##35=å£4': 613, '##36': 614, '##365': 615, '##36504': 616, '##373': 617, '##374': 618, '##3748': 619, '##378': 620, '##38': 621, '##385': 622, '##38767': 623, '##38x': 624, '##39': 625, '##394': 626, '##398': 627, '##3982': 628, '##39822': 629, '##3lp.msg@150p': 630, '##3w': 631, '##3wv': 632, '##3x': 633, '##3xx': 634, '##4': 635, '##4-': 636, '##4-1': 637, '##4-5': 638, '##4-5w': 639, '##4.50/w': 640, '##4/04': 641, '##4/09/02': 642, '##4/10/04': 643, '##4/7': 644, '##40': 645, '##400': 646, '##403': 647, '##405': 648, '##407': 649, '##4071': 650, '##40716': 651, '##407165': 652, '##41': 653, '##41701216+': 654, '##419': 655, '##4190': 656, '##41906': 657, '##419060': 658, '##4190604': 659, '##4190604,': 660, '##4199': 661, '##42': 662, '##420': 663, '##420-': 664, '##4235': 665, '##4235w': 666, '##4235wc': 667, '##4235wc1': 668, '##428': 669, '##42807': 670, '##428070': 671, '##4280703': 672, '##4284': 673, '##43': 674, '##43.': 675, '##43.c': 676, '##434': 677, '##4345': 678, '##4355': 679, '##440': 680, '##44345': 681, '##4477977': 682, '##449': 683, '##45': 684, '##45+': 685, '##45p': 686, '##45pm': 687, '##45w': 688, '##45wq': 689, '##46': 690, '##468': 691, '##47': 692, '##477977': 693, '##478': 694, '##47p': 695, '##48': 696, '##487124': 697, '##49': 698, '##49/': 699, '##49/m': 700, '##498****7': 701, '##4q': 702, '##4q=': 703, '##4w45wq': 704, '##4xx26': 705, '##5': 706, '##5,61,66': 707, '##5/': 708, '##50': 709, '##50(': 710, '##50(m': 711, '##50)': 712, '##50/': 713, '##50/m': 714, '##50/p': 715, '##50/pm': 716, '##50/pm,': 717, '##50/w': 718, '##500': 719, '##50000327': 720, '##506': 721, '##5069': 722, '##51': 723, '##512': 724, '##5120': 725, '##51f': 726, '##52': 727, '##5239': 728, '##526': 729, '##55': 730, '##554': 731, '##5555': 732, '##557': 733, '##5572': 734, '##55727': 735, '##557271': 736, '##5572711': 737, '##5572711&': 738, '##56': 739, '##56669': 740, '##5=å£4': 741, '##5p': 742, '##5p/': 743, '##5p/m': 744, '##5q': 745, '##6': 746, '##6*å£1': 747, '##6+': 748, '##6+å£1': 749, '##6/': 750, '##6/1': 751, '##6/10': 752, '##6/10/': 753, '##6/10/0': 754, '##6/10/04': 755, '##6/11': 756, '##6/11/': 757, '##6/11/0': 758, '##6/11/04': 759, '##60': 760, '##61': 761, '##61,66': 762, '##61x': 763, '##62117': 764, '##634': 765, '##64': 766, '##64x': 767, '##65': 768, '##65,61,66': 769, '##650': 770, '##6504': 771, '##66': 772, '##660': 773, '##6600': 774, '##665': 775, '##6650': 776, '##666': 777, '##6669': 778, '##667': 779, '##674': 780, '##674&': 781, '##678': 782, '##69': 783, '##696': 784, '##6g': 785, '##6gb': 786, '##6gbp': 787, '##6gbp/': 788, '##6gbp/m': 789, '##6gbp/mn': 790, '##6p': 791, '##7': 792, '##7!': 793, '##7!-': 794, '##7!-4': 795, '##7/': 796, '##7/0': 797, '##7/03': 798, '##7/6': 799, '##7/6/': 800, '##7/6/0': 801, '##7/6/03': 802, '##701216+': 803, '##705572711&': 804, '##718': 805, '##73': 806, '##734': 807, '##735=å£4': 808, '##735=å£45': 809, '##74': 810, '##75': 811, '##750': 812, '##75m': 813, '##7678': 814, '##77': 815, '##776': 816, '##7763': 817, '##77632': 818, '##778': 819, '##7781': 820, '##77814': 821, '##778148': 822, '##7781482': 823, '##77814823': 824, '##778148237': 825, '##7781482378': 826, '##779': 827, '##77977': 828, '##77>': 829, '##78': 830, '##782': 831, '##784': 832, '##78498****7': 833, '##7q': 834, '##7qp': 835, '##7qp,': 836, '##7rw18': 837, '##8': 838, '##8\"': 839, '##8****7': 840, '##8+': 841, '##8+)': 842, '##8+6*å£1': 843, '##8,22,65,61,66': 844, '##8,22,65,61,66,': 845, '##8,22,65,61,66,3': 846, '##8,22,65,61,66,38': 847, '##8,22,65,61,66,382': 848, '##8,22,65,61,66,382.': 849, '##8/11/04': 850, '##8/5': 851, '##80': 852, '##801': 853, '##8011': 854, '##80114': 855, '##80155': 856, '##8039': 857, '##804': 858, '##8048': 859, '##80488': 860, '##8066': 861, '##807': 862, '##810': 863, '##81151': 864, '##81403': 865, '##814032': 866, '##819': 867, '##82': 868, '##82050': 869, '##820554': 870, '##822': 871, '##8222': 872, '##82228': 873, '##82228.': 874, '##82228.c': 875, '##82228>>': 876, '##825': 877, '##832156': 878, '##83355': 879, '##836': 880, '##8394': 881, '##84': 882, '##8498****7': 883, '##86': 884, '##866': 885, '##869': 886, '##87': 887, '##870': 888, '##8704': 889, '##871': 890, '##8712': 891, '##87124': 892, '##8714': 893, '##87147': 894, '##871474': 895, '##87187': 896, '##871872': 897, '##8718727': 898, '##871872787': 899, '##8718728': 900, '##871872887': 901, '##8718728876': 902, '##872': 903, '##8726': 904, '##87268': 905, '##872682': 906, '##8726822': 907, '##876': 908, '##8767': 909, '##877': 910, '##877>': 911, '##878': 912, '##882': 913, '##8866': 914, '##8877': 915, '##8877>': 916, '##89': 917, '##89(': 918, '##892': 919, '##8922': 920, '##896': 921, '##8:0430-j': 922, '##8:30': 923, '##8;-)': 924, '##8>>': 925, '##8r7!-4': 926, '##9': 927, '##9(': 928, '##9*378': 929, '##9*3781': 930, '##9*37819': 931, '##9*37819&': 932, '##9/': 933, '##9/0': 934, '##9/02': 935, '##9/03': 936, '##9/03/': 937, '##9/03/0': 938, '##9/03/05': 939, '##9/1': 940, '##9/10': 941, '##9/10/': 942, '##9/10/0': 943, '##9050000327': 944, '##91': 945, '##911(å£': 946, '##91784': 947, '##91ff9*37819&': 948, '##9280114': 949, '##93': 950, '##930': 951, '##9307': 952, '##93076': 953, '##938767': 954, '##95': 955, '##950': 956, '##9503': 957, '##95038': 958, '##9557': 959, '##956669': 960, '##95q': 961, '##97': 962, '##98****7': 963, '##9869': 964, '##99': 965, '##9911(å£': 966, '##:': 967, '##:(': 968, '##:)': 969, '##:)\"': 970, '##:)8': 971, '##:):-):-):-)': 972, '##:-': 973, '##:-(': 974, '##:-)': 975, '##:-)\"': 976, '##:-):': 977, '##:-):-': 978, '##:-):-)': 979, '##:-):-):)': 980, '##:-):-):):-)': 981, '##:-):-):):-).': 982, '##:-):-):-)': 983, '##:-):-d': 984, '##:-):-db-)': 985, '##:-);-)': 986, '##:-);-)b-)': 987, '##:-*': 988, '##:-d;-),': 989, '##:-|': 990, '##://': 991, '##:0430-j': 992, '##:0870': 993, '##:08700': 994, '##:087004': 995, '##:0870043': 996, '##:08700435': 997, '##:087004355': 998, '##:0870043550': 999, '##:08700435505': 1000, '##:087004355051': 1001, '##:0870043550515': 1002, '##:08700435505150': 1003, '##:08700435505150p': 1004, '##:0870046': 1005, '##:08700469': 1006, '##:087004696': 1007, '##:0870046964': 1008, '##:08700469649': 1009, '##:08704': 1010, '##:087040': 1011, '##:0870405': 1012, '##:08704050': 1013, '##:087040504': 1014, '##:0870405040': 1015, '##:08704050406': 1016, '##:08704050406)': 1017, '##:087124': 1018, '##:0871240': 1019, '##:08712400': 1020, '##:087124006': 1021, '##:0871240060': 1022, '##:08712400602': 1023, '##:087124006024': 1024, '##:0871240060245': 1025, '##:08712400602450': 1026, '##:0871872': 1027, '##:08718720': 1028, '##:087187202': 1029, '##:0871872020': 1030, '##:08718720201': 1031, '##:0q': 1032, '##:10': 1033, '##:30': 1034, '##:4xx26': 1035, '##:81151': 1036, '##:83355': 1037, '##:83355!': 1038, '##:9280114': 1039, '##:93076': 1040, '##:930762': 1041, '##:9307622': 1042, '##::': 1043, '##::::': 1044, '##:::::': 1045, '##:\\\\': 1046, '##:\\\\d': 1047, '##:\\\\y': 1048, '##:d;):': 1049, '##:www.100p': 1050, '##;': 1051, '##;#&': 1052, '##;#&g': 1053, '##;)': 1054, '##;):': 1055, '##;-(': 1056, '##;-(.': 1057, '##;-)': 1058, '##;-),': 1059, '##;3': 1060, '##;:(': 1061, '##=': 1062, '##=1': 1063, '##=1b': 1064, '##=1b6': 1065, '##=44345': 1066, '##=820554': 1067, '##=j5q': 1068, '##=q': 1069, '##=qj': 1070, '##=qjk': 1071, '##=qjkg': 1072, '##=qjkgi': 1073, '##=qjkgig': 1074, '##=qjkgigh': 1075, '##=qjkgighj': 1076, '##=qjkgighjj': 1077, '##=qjkgighjjg': 1078, '##=qjkgighjjgc': 1079, '##=qjkgighjjgcb': 1080, '##=qjkgighjjgcbl': 1081, '##=å£': 1082, '##=å£4': 1083, '##>': 1084, '##>2000': 1085, '##>>': 1086, '##>>>': 1087, '##>f': 1088, '##?': 1089, '##?:-|': 1090, '##?\\\\\"\"': 1091, '##?id=1b6': 1092, '##?id=820554': 1093, '##?n=qjkgighjjgcbl': 1094, '##?xx': 1095, '##?ì_': 1096, '##@': 1097, '##@1': 1098, '##@15': 1099, '##@150': 1100, '##@150p': 1101, '##@150p/': 1102, '##@150p/m': 1103, '##@50': 1104, '##@50p': 1105, '##@50p/': 1106, '##@50p/m': 1107, '##@50p/ms': 1108, '##@50p/msg': 1109, '##@k': 1110, '##@ki': 1111, '##@txt82228.c': 1112, '##@vipclub4u.': 1113, '##@å£': 1114, '##@å£1': 1115, '##[': 1116, '##[/': 1117, '##[/c': 1118, '##\\\\': 1119, '##\\\\\"': 1120, '##\\\\\"\"': 1121, '##\\\\\":-)\"': 1122, \"##\\\\'\": 1123, '##]': 1124, '##_': 1125, '##_n': 1126, '##a': 1127, '##a$': 1128, '##a....!!:)': 1129, '##a.3lp.msg@150p': 1130, '##a/150p': 1131, '##a128': 1132, '##a14': 1133, '##a32wu.': 1134, '##a6600': 1135, '##a6650': 1136, '##a7rw18': 1137, '##a:-):': 1138, '##aching/pc1323': 1139, '##ad\\x89û_': 1140, '##aid:\\\\if': 1141, '##al:\\\\d': 1142, '##alary..:-);-)': 1143, '##alf-8': 1144, '##alid12hrs': 1145, '##all09050000327': 1146, '##alls1.50ppm': 1147, '##allså£1/mi': 1148, \"##am)'\": 1149, '##am-11pm': 1150, '##am-7pm': 1151, '##am-9pm': 1152, '##amp;': 1153, '##anding...|': 1154, '##aning:::::': 1155, '##anx4': 1156, '##anyxxxxx': 1157, '##ark.6ph': 1158, '##ars3,': 1159, '##arwars3,': 1160, '##ary..:-);-)': 1161, '##as+m': 1162, '##asq!ih': 1163, '##atrix3,': 1164, '##aughs*\"': 1165, '##ax10mi': 1166, '##ax6/m': 1167, '##axx\\\\\"\"': 1168, '##axå£7.': 1169, '##ay..:)\"': 1170, '##ay.\\\\\":-)\"': 1171, '##ays.ì¬': 1172, '##b': 1173, '##b!4': 1174, '##b-)': 1175, '##b3': 1176, '##b3c': 1177, '##b3ck': 1178, '##b4': 1179, '##b4u': 1180, '##b64x': 1181, '##b>': 1182, '##b>>': 1183, '##bcm4235wc1': 1184, '##bp1.50/w': 1185, '##bp4.50/w': 1186, '##bp5/': 1187, '##bs16+1w': 1188, '##bum-q': 1189, '##bum-qu': 1190, '##by-:0q': 1191, '##c': 1192, '##c/w/icmb3ck': 1193, '##c100p/': 1194, '##c1323': 1195, '##c1n3xx': 1196, '##c2a.3lp.msg@150p': 1197, '##c]': 1198, '##caxx\\\\\"\"': 1199, '##ching/pc1323': 1200, '##cl03530150pm': 1201, '##club4': 1202, '##cm1896wc1': 1203, '##cm4235wc1': 1204, '##cm4284': 1205, '##cm61x': 1206, '##cmb3ck': 1207, '##cmsfwc1n3xx': 1208, '##cs08714740323': 1209, '##cs087147403231w': 1210, '##cvd18': 1211, '##cvd18+': 1212, '##d': 1213, '##d...!\\x8eö´\\x89ó_??\\x8bû¬ud': 1214, '##d....|': 1215, '##d...ìä': 1216, '##d.å£1,50/m': 1217, '##d.å£1,50/mtmsgrcvd18+': 1218, '##d0': 1219, '##d12': 1220, '##d12h': 1221, '##d18': 1222, '##d18+': 1223, '##d3wv': 1224, '##d:\\\\': 1225, '##d;):': 1226, '##d;-),': 1227, '##d=1b6': 1228, '##d=820554': 1229, '##d@150p/m': 1230, '##d@150p/ms': 1231, '##d@150p/msg': 1232, '##d@150p/msg.': 1233, '##d@150p/msg.2': 1234, '##d@50p': 1235, '##day.\\\\\":-)\"': 1236, '##dia:-):': 1237, '##ding...|': 1238, '##dn;-(.': 1239, '##dnw15h': 1240, '##drm-$900...': 1241, '##du.sg/~phyhcmk/': 1242, '##du.sg/~phyhcmk/t': 1243, '##dxxxx': 1244, '##d\\x89û_': 1245, '##e': 1246, '##f': 1247, '##f!\"': 1248, '##f--': 1249, '##f-8': 1250, '##f15': 1251, '##f150p': 1252, '##f42': 1253, '##f4q=': 1254, '##f56': 1255, '##f9*37819&': 1256, '##f91ff9*37819&': 1257, '##f91ff9*37819&f': 1258, '##f91ff9*37819&fi': 1259, '##f91ff9*37819&fir': 1260, '##f91ff9*37819&firs': 1261, '##f:-(': 1262, '##f:9280114': 1263, '##f:9307622': 1264, '##ff!\"': 1265, '##ff--': 1266, '##ff--w': 1267, '##ff42': 1268, '##ff42m': 1269, '##ff9*37819&': 1270, '##fly150ppm': 1271, '##fwc1n3xx': 1272, '##fwfly150ppm': 1273, '##få£2000': 1274, '##g': 1275, '##g+': 1276, '##g...|': 1277, '##g..|': 1278, '##g.23f': 1279, '##g.23g': 1280, '##g/pc1323': 1281, '##g/~p': 1282, '##g/~ph': 1283, '##g/~phy': 1284, '##g/~phyh': 1285, '##g/~phyhc': 1286, '##g/~phyhcm': 1287, '##g/~phyhcmk': 1288, '##g/~phyhcmk/': 1289, '##g11': 1290, '##g150p': 1291, '##g18': 1292, '##g21': 1293, '##g:-)\"': 1294, '##g:::::': 1295, '##g>f': 1296, '##g@150p': 1297, '##g@å£1': 1298, '##g@å£1.50': 1299, '##gd@50p': 1300, '##ghs*\"': 1301, '##ght!\"': 1302, '##ghtåóbraindanc': 1303, '##glsuplt)10,': 1304, '##grcvd18': 1305, '##grcvd18+': 1306, '##gs:d;):': 1307, '##gs@150p': 1308, '##gt;:(': 1309, '##h': 1310, '##h11': 1311, '##h74': 1312, '##h:08700435505150p': 1313, '##h:08704050406)': 1314, '##hanx4': 1315, '##hikku:-):-db-)': 1316, '##hikku:-);-)b-)': 1317, '##hing/pc1323': 1318, '##hit....!!\\\\\"': 1319, '##hldxxxx': 1320, '##hnåó-s': 1321, '##hp51f': 1322, '##hrgd@50p': 1323, '##hs*\"': 1324, '##hs+?': 1325, '##ht!\"': 1326, '##htåóbraindanc': 1327, '##hxxxxxxxxxxx': 1328, '##hy@k': 1329, '##hy@ki': 1330, '##i': 1331, '##i!!!!': 1332, '##i8\"': 1333, '##i8;-)': 1334, '##ia/150p': 1335, '##ia6600': 1336, '##ia6650': 1337, '##ia:-):': 1338, '##ic]': 1339, '##icmb3ck': 1340, '##id12h': 1341, '##id:\\\\': 1342, '##id:\\\\i': 1343, '##id:\\\\if': 1344, '##id=1b6': 1345, '##id=820554': 1346, '##ig..|': 1347, '##ight!\"': 1348, '##ikku:-):-db-)': 1349, '##ikku:-);-)b-)': 1350, '##in-82050.c': 1351, '##in150ppmx3': 1352, '##ing...|': 1353, '##ing/pc1323': 1354, '##ing:::::': 1355, '##ini!!!!': 1356, '##ins&100': 1357, '##ipclub4u.': 1358, '##it....!!\\\\\"': 1359, '##ivby-:0qu': 1360, '##ivby-:0qui': 1361, '##ix3,': 1362, '##ix\\\\\"': 1363, '##iz10ppm': 1364, '##j': 1365, '##j5q': 1366, '##j6': 1367, '##j9': 1368, '##k': 1369, '##k!\\\\\"\"': 1370, '##k+': 1371, '##k.220cm2': 1372, '##k.6p': 1373, '##k.6ph': 1374, '##k.|': 1375, '##k1510': 1376, '##k1510.': 1377, '##k1510..': 1378, '##k1510...': 1379, '##k3': 1380, '##k38': 1381, '##k38c': 1382, '##k38ch': 1383, '##k38w': 1384, '##k38wp': 1385, '##k38wp150pp': 1386, '##k38wp150ppm': 1387, '##k38wp150ppm18+': 1388, '##k38x': 1389, '##k45': 1390, '##kia/150p': 1391, '##kia6600': 1392, '##kia6650': 1393, '##kku:-):-db-)': 1394, '##kku:-);-)b-)': 1395, '##kp>2000': 1396, '##ku:-):-db-)': 1397, '##ku:-);-)b-)': 1398, '##l': 1399, '##l!!!!!!!!!': 1400, '##l&g': 1401, '##l&gt': 1402, '##l&gt;': 1403, '##l..:-d;-),': 1404, '##l03530150pm': 1405, '##l09050000327': 1406, '##l341.': 1407, '##l:-):-):):-).': 1408, '##l:\\\\d': 1409, '##l?id=1b6': 1410, '##l?id=820554': 1411, '##lary..:-);-)': 1412, '##laughs*\"': 1413, '##lbum-qu': 1414, '##ldxxxx': 1415, '##lf-8': 1416, '##lf56': 1417, '##lf:-(': 1418, '##lid12h': 1419, '##lid12hr': 1420, '##lid12hrs': 1421, '##ll..:-d;-),': 1422, '##ll09050000327': 1423, '##lls1.50ppm': 1424, '##llså£1/m': 1425, '##llså£1/mi': 1426, '##lp.msg@150p': 1427, '##lp08700621170150p': 1428, '##lp08714742804': 1429, '##lp08718728876': 1430, '##lp:08700469649.': 1431, '##lp:08712400602450p': 1432, '##ls1.50ppm': 1433, '##lsuplt)10,': 1434, '##lså£1/m': 1435, '##lt)10,': 1436, '##lt;#&g': 1437, '##lt;3': 1438, '##lub4': 1439, '##lub>>': 1440, '##lwa....!!:)': 1441, '##ly#': 1442, '##ly/200p': 1443, '##ly150ppm': 1444, '##ly>>>': 1445, '##lys150': 1446, '##m': 1447, '##m!!!': 1448, \"##m)'\": 1449, '##m-$900': 1450, '##m-$900.': 1451, '##m-$900..': 1452, '##m-$900...': 1453, '##m-11p': 1454, '##m-11pm': 1455, '##m-7p': 1456, '##m-7pm': 1457, '##m-9p': 1458, '##m-9pm': 1459, '##m-q': 1460, '##m.subs16+1w': 1461, '##m.subs16+1wi': 1462, '##m.subs16+1win150ppmx3': 1463, '##m12': 1464, '##m150': 1465, '##m150p/': 1466, '##m1896wc1': 1467, '##m1win150ppmx3': 1468, '##m1win150ppmx3a': 1469, '##m1win150ppmx3ag': 1470, '##m4235wc1': 1471, '##m4284': 1472, '##m61x': 1473, '##m?n=qjkgighjjgcbl': 1474, '##mb3ck': 1475, '##mb64x': 1476, '##mix\\\\\"': 1477, '##ml?id=1b6': 1478, '##ml?id=820554': 1479, '##mp;': 1480, '##ms-08': 1481, '##ms-08718727870': 1482, '##ms-08718727870150ppm': 1483, '##msfwc1n3xx': 1484, '##msg18': 1485, '##msg>f': 1486, '##msg>fa': 1487, '##msg>fav': 1488, '##msg@150p': 1489, '##msgrcvd18': 1490, '##msgrcvd18+': 1491, '##muk.220cm2': 1492, '##mx3': 1493, '##n': 1494, '##n-82050.c': 1495, '##n0819': 1496, '##n150ppmx3': 1497, '##n3xx': 1498, '##n5120': 1499, '##n;-(.': 1500, '##n=qjkgighjjgcbl': 1501, '##n@50p/msg': 1502, \"##n\\\\'\": 1503, \"##n\\\\'t\": 1504, '##ndia:-):': 1505, '##nding...|': 1506, '##ng...|': 1507, '##ng/pc1323': 1508, '##ng:-)\"': 1509, '##ng:::::': 1510, '##ni!!!!': 1511, '##ning:::::': 1512, '##ns&100': 1513, '##nw15h': 1514, '##nx4': 1515, '##nyxxxxx': 1516, '##n\\x89ûª': 1517, '##nåó-s': 1518, '##o': 1519, '##p': 1520, '##p+': 1521, '##p.msg@150p': 1522, '##p//': 1523, '##p//w': 1524, '##p//ww': 1525, '##p//www': 1526, '##p//www.': 1527, '##p//www.g': 1528, '##p//www.gr': 1529, '##p//www.gr8': 1530, '##p//www.gr8p': 1531, '##p//www.gr8pr': 1532, '##p//www.gr8pri': 1533, '##p//www.gr8priz': 1534, '##p08700621170150': 1535, '##p08700621170150p': 1536, '##p08714742804': 1537, '##p08718728876': 1538, '##p1.50/w': 1539, '##p176781': 1540, '##p20': 1541, '##p3': 1542, '##p4': 1543, '##p4-5w': 1544, '##p4.50/w': 1545, '##p5/': 1546, '##p51f': 1547, '##p://': 1548, '##p://c': 1549, '##p://ca': 1550, '##p://car': 1551, '##p://w': 1552, '##p://wa': 1553, '##p://wap': 1554, '##p://wap.': 1555, '##p://ww': 1556, '##p://www': 1557, '##p://www.': 1558, '##p://www.b': 1559, '##p://www.bu': 1560, '##p://www.bub': 1561, '##p://www.bubb': 1562, '##p://www.bubbl': 1563, '##p://www.u': 1564, '##p://www.ur': 1565, '##p://www.ura': 1566, '##p://www.uraw': 1567, '##p://www.urawi': 1568, '##p://www.urawin': 1569, '##p://www.urawinn': 1570, '##p://www.v': 1571, '##p://www.w': 1572, '##p:08700469649': 1573, '##p:08700469649.': 1574, '##p:08712400602450': 1575, '##p:08712400602450p': 1576, '##p;': 1577, '##p>2000': 1578, '##pc1323': 1579, '##pclub4': 1580, '##pclub4u': 1581, '##pclub4u.': 1582, '##plt)10,': 1583, '##plys150': 1584, '##pm150': 1585, '##psms-08': 1586, '##psms-08718727870150ppm': 1587, '##pt150x3+n': 1588, '##på£1.50/w': 1589, '##q': 1590, '##q!': 1591, '##q825': 1592, '##q825,': 1593, '##q=': 1594, '##qu': 1595, '##quiz10ppm': 1596, '##qxj9': 1597, '##r': 1598, '##r!\\\\\"\"': 1599, '##r01327b': 1600, \"##r18's\": 1601, '##r31': 1602, '##r7!-4': 1603, '##ral:\\\\d': 1604, '##ranyxxxxx': 1605, '##rcvd18': 1606, '##rcvd18+': 1607, '##rd...ìä': 1608, '##rgd@50p': 1609, '##rix3,': 1610, '##rk!\\\\\"\"': 1611, '##rk.6ph': 1612, '##rm-$900...': 1613, '##rm150p/': 1614, '##rm150p/t': 1615, '##rng:-)\"': 1616, '##rs3,': 1617, '##rsglsuplt)10,': 1618, '##rw18': 1619, '##rwars3,': 1620, '##ry..:-);-)': 1621, '##ry41': 1622, '##rå£38': 1623, '##s': 1624, '##s&100': 1625, '##s&cs': 1626, '##s*\"': 1627, '##s+?': 1628, '##s+m': 1629, '##s+mu': 1630, '##s+mus': 1631, '##s+musi': 1632, '##s+music': 1633, '##s+musicn': 1634, '##s-08': 1635, '##s-08718727870': 1636, '##s-08718727870150ppm': 1637, '##s..!!!;-)': 1638, '##s.ì¬': 1639, '##s08714740323': 1640, '##s087147403231w': 1641, '##s1.50ppm': 1642, '##s150': 1643, '##s15hb': 1644, '##s16+1w': 1645, '##s278bb': 1646, '##s3,': 1647, '##s7': 1648, '##s8,22,65,61,66,382.': 1649, '##s:d;):': 1650, '##s@150p': 1651, '##sbcm4235wc1': 1652, '##scs08714740323': 1653, '##scs087147403231w': 1654, '##scs087147403231wi': 1655, '##scs087147403231win': 1656, '##scs087147403231wina': 1657, '##scs087147403231winaw': 1658, '##scs087147403231winawk': 1659, '##scs087147403231winawk!': 1660, '##scs087147403231winawk!a': 1661, '##scs087147403231winawk!ag': 1662, '##sfwc1n3xx': 1663, '##sg+': 1664, '##sg/~phyhcmk/': 1665, '##sg150p': 1666, '##sg18': 1667, '##sg>f': 1668, '##sg@150p': 1669, '##sg@å£1.50': 1670, '##sglsuplt)10,': 1671, '##sgrcvd18': 1672, '##sgrcvd18+': 1673, '##sgs:d;):': 1674, '##sgs@150p': 1675, '##sh11': 1676, '##shxxxxxxxxxxx': 1677, '##sic]': 1678, '##sk38ch': 1679, '##sms-08': 1680, '##sms-08718727870150ppm': 1681, '##sn\\x89ûª': 1682, '##sq!': 1683, '##sq!i': 1684, '##sq!ih': 1685, '##stå£1.50/pm,': 1686, '##subs16+1w': 1687, '##suplt)10,': 1688, '##s\\x89ûów': 1689, '##så£1/m': 1690, '##t': 1691, '##t!\"': 1692, '##t!)xx': 1693, '##t&c': 1694, '##t&cs': 1695, '##t)10,': 1696, '##t+b': 1697, '##t-2-sh': 1698, '##t....!!\\\\\"': 1699, '##t/120p': 1700, '##t/150p16+': 1701, '##t/4qf2': 1702, '##t/d3wv': 1703, '##t/f4q=': 1704, '##t/j': 1705, '##t/lf56': 1706, '##t/ì¼1.20': 1707, '##t150x3+n': 1708, '##t1jy': 1709, '##t250.c': 1710, '##t43.c': 1711, '##t80155': 1712, '##t82228.c': 1713, '##t82228>>': 1714, '##t:-*': 1715, '##t;#&g': 1716, '##t;3': 1717, '##t;:(': 1718, '##t=j5q': 1719, '##t?ì_': 1720, '##t[/c': 1721, \"##tam)'\": 1722, '##tanding...|': 1723, '##tarwars3,': 1724, '##thy@ki': 1725, '##tmsgrcvd18': 1726, '##tmsgrcvd18+': 1727, '##tp://': 1728, '##tp://car': 1729, '##tp://wap.': 1730, '##tp://www.': 1731, '##tp://www.bubbl': 1732, '##tp://www.urawinn': 1733, '##trix3,': 1734, '##try41': 1735, '##ts&cs': 1736, '##tscs087147403231winawk!ag': 1737, \"##ttam)'\": 1738, '##ttp://': 1739, '##ttp://car': 1740, '##ttp://wap.': 1741, '##ttp://www.': 1742, '##ttp://www.bubbl': 1743, '##ttp://www.urawinn': 1744, '##tuff!\"': 1745, '##tuff42m': 1746, '##txt-2-sh': 1747, '##txt/120p': 1748, '##txt/ì¼1.20': 1749, '##txt43.c': 1750, '##txt82228.c': 1751, '##t~j': 1752, '##tå£1.50/pm,': 1753, '##tåóbraindanc': 1754, '##u': 1755, '##u.sg/~phyhcmk/': 1756, '##u:-):-db-)': 1757, '##u:-);-)b-)': 1758, '##ub4': 1759, '##ub>>': 1760, '##ubs16+1w': 1761, '##udn;-(.': 1762, '##uff!\"': 1763, '##uff--w': 1764, '##uff--wh': 1765, '##uff--why': 1766, '##uff42m': 1767, '##ughs*\"': 1768, '##ughtåóbraindanc': 1769, '##uk.220cm2': 1770, '##um-q': 1771, '##un0819': 1772, '##uplt)10,': 1773, '##us8,22,65,61,66,382.': 1774, '##ut!)xx': 1775, '##ut/4qf2': 1776, '##ut/d3wv': 1777, '##ut/f4q=': 1778, '##ut/j': 1779, '##ut/lf56': 1780, '##ut=j5q': 1781, '##uzzzz!': 1782, '##v': 1783, '##vby-:0q': 1784, '##vby-:0qu': 1785, '##vd18': 1786, '##vd18+': 1787, '##vipclub4u.': 1788, '##w': 1789, '##w!\"': 1790, '##w!4': 1791, '##w!4t': 1792, '##w!4t&': 1793, '##w!t&cs': 1794, '##w.07781482378': 1795, '##w.100p': 1796, '##w.4-': 1797, '##w.80488': 1798, '##w.b4u': 1799, '##w.txt-2-sh': 1800, '##w.txt43.c': 1801, '##w.txt82228.c': 1802, '##w.win-82050.c': 1803, '##w/icmb3ck': 1804, '##w/w1j': 1805, '##w/w1j6': 1806, '##w/w1j6h': 1807, '##w/w1j6hl': 1808, '##w/w1jh': 1809, '##w/w1jhl': 1810, '##w15': 1811, '##w15h': 1812, '##w18': 1813, '##w1a7rw18': 1814, '##w1j': 1815, '##w1j6': 1816, '##w25': 1817, '##w25w': 1818, '##w25wx': 1819, '##w45wq': 1820, '##w7': 1821, '##w73': 1822, '##wa....!!:)': 1823, '##wars3,': 1824, '##was+m': 1825, '##wc1n3xx': 1826, '##wfly150ppm': 1827, '##win-82050.c': 1828, '##win150ppmx3': 1829, '##ww.07781482378': 1830, '##ww.100p': 1831, '##ww.4-': 1832, '##ww.80488': 1833, '##ww.b4u': 1834, '##ww.txt-2-sh': 1835, '##ww.txt43.c': 1836, '##ww.txt82228.c': 1837, '##ww.win-82050.c': 1838, '##www.100p': 1839, '##wxxxx': 1840, '##x': 1841, '##x10': 1842, '##x10183': 1843, '##x10183b': 1844, '##x10183bh': 1845, '##x10183bha': 1846, '##x10183bhamb64x': 1847, '##x10m': 1848, '##x10mi': 1849, '##x114/14': 1850, '##x114/14t': 1851, '##x114/14tc': 1852, '##x114/14tcr': 1853, '##x114/14tcr/': 1854, '##x114/14tcr/w': 1855, '##x114/14tcr/w1': 1856, '##x1146': 1857, '##x12': 1858, '##x1282': 1859, '##x12n': 1860, '##x12n146': 1861, '##x12n146t': 1862, '##x12n146tf15': 1863, '##x12n146tf150p': 1864, '##x139': 1865, '##x139,': 1866, '##x150p/': 1867, '##x177': 1868, '##x177.': 1869, '##x177hp51f': 1870, '##x177hp51fl': 1871, '##x202': 1872, '##x245': 1873, '##x245c': 1874, '##x245c2150p': 1875, '##x245c2150pm': 1876, '##x26': 1877, '##x3': 1878, '##x3+': 1879, '##x3,': 1880, '##x326': 1881, '##x334': 1882, '##x334,': 1883, '##x334sk38ch': 1884, '##x365': 1885, '##x36504': 1886, '##x36504w': 1887, '##x36504w45': 1888, '##x36504w45w': 1889, '##x36504w45wq': 1890, '##x3748': 1891, '##x385': 1892, '##x39822': 1893, '##x4': 1894, '##x403': 1895, '##x42': 1896, '##x420': 1897, '##x420-': 1898, '##x420.': 1899, '##x42w': 1900, '##x42wr': 1901, '##x42wr29c,': 1902, '##x434': 1903, '##x434s': 1904, '##x434sk38wp150ppm18+': 1905, '##x45': 1906, '##x45p': 1907, '##x45w': 1908, '##x45w2': 1909, '##x45w2t': 1910, '##x45w2tg150p': 1911, '##x526': 1912, '##x526,': 1913, '##x6/': 1914, '##x6/m': 1915, '##x61': 1916, '##x61,': 1917, '##x61,m': 1918, '##x61,m60': 1919, '##x734': 1920, '##x734l': 1921, '##x734ls': 1922, '##x734ls27yf': 1923, '##x75': 1924, '##x75l': 1925, '##x75ld': 1926, '##x75ldn': 1927, '##x75ldns7': 1928, '##x84': 1929, '##x84,': 1930, '##x95q': 1931, '##x95qu': 1932, '##x97': 1933, '##x97n': 1934, '##x97n7qp,': 1935, '##x\\\\\"': 1936, '##x\\\\\"\"': 1937, '##xcm61x': 1938, '##xcm61xn': 1939, '##xj9': 1940, '##xt-2-sh': 1941, '##xt/120p': 1942, '##xt/ì¼1.20': 1943, '##xt250.c': 1944, '##xt43.c': 1945, '##xt82228.c': 1946, '##xt82228>>': 1947, '##xt[/c': 1948, '##xt~j': 1949, '##xx': 1950, '##xx26': 1951, '##xx\\\\\"\"': 1952, '##xxx': 1953, '##xxx\\\\\"\"': 1954, '##xxxx': 1955, '##xxxxx': 1956, '##xxxxxxxx': 1957, '##xxxxxxxxxx': 1958, '##xxxxxxxxxxx': 1959, '##xå£150': 1960, '##xå£7': 1961, '##xå£7.': 1962, '##y': 1963, '##y#': 1964, '##y-:0q': 1965, '##y..:)\"': 1966, '##y..:-);-)': 1967, '##y.\\\\\":-)\"': 1968, '##y/200p': 1969, '##y150ppm': 1970, '##y41': 1971, '##y>>>': 1972, '##y@k': 1973, '##ys.ì¬': 1974, '##ys150': 1975, '##yt:-*': 1976, '##yxxxxx': 1977, '##yåó': 1978, '##z': 1979, '##z10': 1980, '##z10p': 1981, '##z10pp': 1982, '##z10ppm': 1983, '##z8r7!-4': 1984, '##zz': 1985, '##zzzz': 1986, '##zzzz!': 1987, '##|': 1988, '##~': 1989, '##~j': 1990, '##\\x89': 1991, '##\\x89û': 1992, '##\\x89û_': 1993, '##\\x89ûª': 1994, '##\\x89ûªm': 1995, '##\\x89ûó': 1996, '##\\x89ûów': 1997, '##\\x89û÷': 1998, '##\\x8b': 1999, '##\\x8bû': 2000, '##\\x8bû¬': 2001, '##\\x8e': 2002, '##\\x8eö': 2003, '##\\x8eö´': 2004, '##\\x8eö´\\x89': 2005, '##\\x8eö´\\x89ó': 2006, '##\\x8eö´\\x89ó_': 2007, '##£': 2008, '##©': 2009, '##ª': 2010, '##¬': 2011, '##´': 2012, '##¼': 2013, '##á': 2014, '##â': 2015, '##ä': 2016, '##å': 2017, '##å£': 2018, '##å£1': 2019, '##å£1,50/': 2020, '##å£1,50/m': 2021, '##å£1.': 2022, '##å£1.50': 2023, '##å£1.50/pm,': 2024, '##å£1.50/w': 2025, '##å£1/': 2026, '##å£1/m': 2027, '##å£150': 2028, '##å£2': 2029, '##å£20': 2030, '##å£200': 2031, '##å£2000': 2032, '##å£3': 2033, '##å£3.': 2034, '##å£3.75m': 2035, '##å£3.75ma': 2036, '##å£3.75max': 2037, '##å£38': 2038, '##å£7': 2039, '##åá': 2040, '##åác': 2041, '##åó': 2042, '##åó-': 2043, '##åó-s': 2044, '##åóa': 2045, '##åób': 2046, '##åóbr': 2047, '##åóbra': 2048, '##åóbrai': 2049, '##åóbrain': 2050, '##åóbraind': 2051, '##åóbrainda': 2052, '##åóbraindan': 2053, '##åóbraindanc': 2054, '##åõ': 2055, '##è': 2056, '##ì': 2057, '##ì_': 2058, '##ì¬': 2059, '##ì¼': 2060, '##ìâ': 2061, '##ìä': 2062, '##ï': 2063, '##ð': 2064, '##ò': 2065, '##ó': 2066, '##ô': 2067, '##õ': 2068, '##ö': 2069, '##÷': 2070, '##û': 2071, '#150': 2072, '#5': 2073, '#50': 2074, '#500': 2075, '#5000': 2076, '$': 2077, '$1': 2078, '$14': 2079, '$140': 2080, '$18': 2081, '$180': 2082, '$2': 2083, '$3': 2084, '$350': 2085, '$5': 2086, '$5.00': 2087, '$50': 2088, '$7': 2089, '$70': 2090, '$700': 2091, '$9': 2092, '$90': 2093, '$900': 2094, '$95': 2095, '$95/': 2096, '$95/p': 2097, '$95/pa': 2098, '$95/pax': 2099, '$95/pax,': 2100, '%': 2101, '&': 2102, '&amp;': 2103, '&gt;:(': 2104, '&lt;#&g': 2105, '&lt;#&gt': 2106, '&lt;#&gt;': 2107, '&lt;3': 2108, '&xxx': 2109, \"'\": 2110, \"''\": 2111, '(': 2112, '($': 2113, '($9': 2114, '($90': 2115, '($900': 2116, '($900)': 2117, '(10': 2118, '(100': 2119, '(100p': 2120, '(100p/': 2121, '(100p/s': 2122, '(100p/sm': 2123, '(100p/sms': 2124, '(100p/sms)': 2125, '(10p': 2126, '(10p/': 2127, '(10p/m': 2128, '(10p/mi': 2129, '(10p/min': 2130, '(10p/min)': 2131, '(150': 2132, '(150p': 2133, '(150p/': 2134, '(150p/s': 2135, '(150p/sm': 2136, '(150p/sms': 2137, '(150p/sms)': 2138, '(18': 2139, '(18+)': 2140, '(20': 2141, '(20/': 2142, '(20/f': 2143, '(20/f)': 2144, '(25': 2145, '(25/': 2146, '(25/f': 2147, '(25/f)': 2148, '(25p': 2149, '(25p)': 2150, '(25p),': 2151, '(29/': 2152, '(29/m': 2153, '(29/m)': 2154, '(32': 2155, '(32/': 2156, '(32/f': 2157, '(32/f)': 2158, '(q': 2159, '(qu': 2160, '(qui': 2161, '(quiz': 2162, '(quizc': 2163, '(quizcl': 2164, '(quizclu': 2165, '(quizclub': 2166, '(\\x89û': 2167, '(\\x89û_': 2168, '(\\x89û_)': 2169, '(å£': 2170, '(å£4': 2171, '(å£4.': 2172, '(å£4.50)': 2173, ')': 2174, '),': 2175, '*': 2176, '**': 2177, '*****': 2178, '***********': 2179, '***************': 2180, '*****u': 2181, '*****up': 2182, '**f': 2183, '**fr': 2184, '*9': 2185, '*laughs*\"': 2186, '+': 2187, '+123': 2188, '+4477977': 2189, '+44779770': 2190, '+447797706': 2191, '+4477977060': 2192, '+44779770600': 2193, '+447797706009': 2194, '+449': 2195, '+4490': 2196, '+44907': 2197, '+449071': 2198, '+4490715': 2199, '+44907151': 2200, '+449071512': 2201, '+4490715124': 2202, '+44907151243': 2203, '+449071512431': 2204, '+å£': 2205, '+å£4': 2206, '+å£40': 2207, '+å£400': 2208, ',': 2209, '-': 2210, '-)': 2211, '.': 2212, '/': 2213, '/-': 2214, '/7': 2215, '0': 2216, '008704': 2217, '0087040': 2218, '00870405': 2219, '008704050': 2220, '0087040504': 2221, '00870405040': 2222, '008704050406': 2223, '0089(': 2224, '0089(m': 2225, '0089(my': 2226, '01': 2227, '012': 2228, '0121': 2229, '0122': 2230, '01223': 2231, '012235': 2232, '0122358': 2233, '01223585': 2234, '012235852': 2235, '0122358523': 2236, '01223585236': 2237, '012235853': 2238, '0122358533': 2239, '01223585334': 2240, '0125': 2241, '01256': 2242, '012569': 2243, '0125698': 2244, '01256987': 2245, '012569878': 2246, '0125698789': 2247, '02': 2248, '02/': 2249, '02/0': 2250, '02/06': 2251, '02/06/': 2252, '02/06/0': 2253, '02/06/03': 2254, '02/06/03!': 2255, '02/09': 2256, '02/09/': 2257, '02/09/0': 2258, '02/09/03': 2259, '02/09/03!': 2260, '020': 2261, '0207': 2262, '0207-': 2263, '0207-0': 2264, '0207-08': 2265, '0207-083': 2266, '0207-083-': 2267, '0207-083-6': 2268, '0207-083-60': 2269, '0207-083-608': 2270, '0207-083-6089': 2271, '02072': 2272, '020720': 2273, '0207206': 2274, '02072069': 2275, '020720694': 2276, '0207206940': 2277, '02072069400': 2278, '02072069400.': 2279, '02073': 2280, '020731': 2281, '0207316': 2282, '02073162': 2283, '020731624': 2284, '0207316241': 2285, '02073162414': 2286, '0208': 2287, '02085': 2288, '020850': 2289, '0208507': 2290, '02085076': 2291, '020850769': 2292, '0208507697': 2293, '02085076972': 2294, '021': 2295, '05': 2296, '050': 2297, '0507': 2298, '05070': 2299, '050703': 2300, '057': 2301, '0578': 2302, '06': 2303, '06.': 2304, '06.05': 2305, '06.05.': 2306, '06.05.05': 2307, '06/': 2308, '06/1': 2309, '06/11': 2310, '06/11/': 2311, '06/11/0': 2312, '06/11/04': 2313, '07': 2314, '07/': 2315, '07/1': 2316, '07/11': 2317, '07/11/': 2318, '07/11/0': 2319, '07/11/04': 2320, '070': 2321, '0700': 2322, '07008': 2323, '070080': 2324, '0700800': 2325, '07008009': 2326, '070080092': 2327, '0700800920': 2328, '07008009200': 2329, '0704': 2330, '07046': 2331, '070467': 2332, '0704674': 2333, '07046744': 2334, '070467444': 2335, '0704674443': 2336, '07046744435': 2337, '0709': 2338, '07090': 2339, '070902': 2340, '0709020': 2341, '07090201': 2342, '070902015': 2343, '0709020152': 2344, '07090201529': 2345, '0709029': 2346, '07090298': 2347, '070902989': 2348, '0709029892': 2349, '07090298926': 2350, '07099': 2351, '070998': 2352, '0709983': 2353, '07099833': 2354, '070998336': 2355, '0709983360': 2356, '07099833605': 2357, '071': 2358, '0712': 2359, '07123': 2360, '071234': 2361, '0712345': 2362, '07123456': 2363, '071234567': 2364, '0712345678': 2365, '07123456789': 2366, '072': 2367, '0721': 2368, '07210': 2369, '072107': 2370, '0721072': 2371, '077': 2372, '0773': 2373, '07732': 2374, '077325': 2375, '0773258': 2376, '07732584': 2377, '077325843': 2378, '0773258435': 2379, '07732584351': 2380, '07734': 2381, '077343': 2382, '0773439': 2383, '07734396': 2384, '077343968': 2385, '0773439683': 2386, '07734396839': 2387, '0774': 2388, '07742': 2389, '077426': 2390, '0774267': 2391, '07742676': 2392, '077426769': 2393, '0774267696': 2394, '07742676969': 2395, '0775': 2396, '07753': 2397, '077537': 2398, '0775374': 2399, '07753741': 2400, '077537412': 2401, '0775374122': 2402, '07753741225': 2403, '0776': 2404, '0776x': 2405, '0776xx': 2406, '0776xxx': 2407, '0776xxxx': 2408, '0776xxxxx': 2409, '0776xxxxxx': 2410, '0776xxxxxxx': 2411, '0778': 2412, '07786': 2413, '077862': 2414, '0778620': 2415, '07786200': 2416, '077862001': 2417, '0778620011': 2418, '07786200117': 2419, '077x': 2420, '077xx': 2421, '077xxx': 2422, '078': 2423, '0780': 2424, '07801': 2425, '078015': 2426, '0780154': 2427, '07801543': 2428, '078015434': 2429, '0780154348': 2430, '07801543489': 2431, '07808': 2432, '078082': 2433, '0780824': 2434, '07808247': 2435, '078082478': 2436, '0780824786': 2437, '07808247860': 2438, '07808726822': 2439, '0781': 2440, '07815': 2441, '078152': 2442, '0781529': 2443, '07815296': 2444, '078152964': 2445, '0781529648': 2446, '07815296484': 2447, '0782': 2448, '07821': 2449, '078212': 2450, '0782123': 2451, '07821230': 2452, '078212309': 2453, '0782123090': 2454, '07821230901': 2455, '078498****7': 2456, '0789': 2457, '0789x': 2458, '0789xx': 2459, '0789xxx': 2460, '0789xxxx': 2461, '0789xxxxx': 2462, '0789xxxxxx': 2463, '0789xxxxxxx': 2464, '0789xxxxxxx.': 2465, '079': 2466, '0794': 2467, '07946': 2468, '079467': 2469, '0794674': 2470, '07946746': 2471, '079467462': 2472, '0794674629': 2473, '07946746291': 2474, '07946746291/': 2475, '07946746291/0': 2476, '07946746291/07': 2477, '07946746291/078': 2478, '07946746291/0788': 2479, '07946746291/07880': 2480, '07946746291/078808': 2481, '07946746291/0788086': 2482, '07946746291/07880867': 2483, '07946746291/078808678': 2484, '07946746291/0788086786': 2485, '07946746291/07880867867': 2486, '0796': 2487, '0796x': 2488, '0796xx': 2489, '0796xxx': 2490, '0796xxxx': 2491, '0796xxxxx': 2492, '0796xxxxxx': 2493, '0796xxxxxx.': 2494, '0797': 2495, '07973': 2496, '079737': 2497, '0797378': 2498, '07973788': 2499, '079737882': 2500, '0797378824': 2501, '07973788240': 2502, '07x': 2503, '07xx': 2504, '07xxx': 2505, '07xxxx': 2506, '07xxxxx': 2507, '07xxxxxx': 2508, '07xxxxxxx': 2509, '07xxxxxxxx': 2510, '07xxxxxxxxx': 2511, '08': 2512, '080': 2513, '0800': 2514, '08000407165': 2515, '0800077632': 2516, '08000776320': 2517, '080008394': 2518, '0800083940': 2519, '08000839402': 2520, '080009307': 2521, '0800093070': 2522, '08000930705': 2523, '08000938767': 2524, '0800195038': 2525, '08001950382': 2526, '08002': 2527, '080028': 2528, '0800288': 2529, '08002888': 2530, '080028888': 2531, '0800288881': 2532, '08002888812': 2533, '080029': 2534, '0800298': 2535, '08002986': 2536, '080029860': 2537, '0800298603': 2538, '08002986030': 2539, '080029869': 2540, '0800298690': 2541, '08002986906': 2542, '08002988': 2543, '080029888': 2544, '0800298889': 2545, '08002988890': 2546, '08006': 2547, '080063': 2548, '0800634': 2549, '08006344': 2550, '080063444': 2551, '0800634444': 2552, '08006344447': 2553, '0808': 2554, '08081': 2555, '080812': 2556, '0808126': 2557, '08081263': 2558, '080812630': 2559, '0808126300': 2560, '08081263000': 2561, '080815': 2562, '0808156': 2563, '08081560': 2564, '080815606': 2565, '0808156066': 2566, '08081560665': 2567, '082': 2568, '0825': 2569, '084': 2570, '0844': 2571, '08448': 2572, '084483': 2573, '0844835': 2574, '08448350': 2575, '084483500': 2576, '0844835005': 2577, '08448350055': 2578, '084487': 2579, '0844871': 2580, '08448714': 2581, '084487141': 2582, '0844871418': 2583, '08448714184': 2584, '0845': 2585, '08450': 2586, '084505': 2587, '0845054': 2588, '08450542': 2589, '084505428': 2590, '0845054283': 2591, '08450542832': 2592, '08452': 2593, '084528': 2594, '0845281': 2595, '08452810': 2596, '084528100': 2597, '0845281007': 2598, '08452810071': 2599, '08452810073': 2600, '08452810075': 2601, '087': 2602, '0870': 2603, '0870062117': 2604, '08700621170': 2605, '08700621170150': 2606, '08700621170150p': 2607, '08701': 2608, '087012': 2609, '0870121': 2610, '08701213': 2611, '087012131': 2612, '0870121318': 2613, '08701213186': 2614, '08701213186.': 2615, '0870123': 2616, '08701237': 2617, '087012373': 2618, '0870123739': 2619, '08701237397': 2620, '087014': 2621, '0870141': 2622, '08701417': 2623, '087014170': 2624, '0870141701': 2625, '08701417012': 2626, '087014170121': 2627, '0870141701215': 2628, '08701417012150': 2629, '08701417012150p': 2630, '0870141701216+': 2631, '087016': 2632, '0870162': 2633, '08701624': 2634, '087016248': 2635, '087017': 2636, '0870175': 2637, '08701752': 2638, '087017525': 2639, '0870175256': 2640, '08701752560': 2641, '08701752560.': 2642, '087018': 2643, '0870187': 2644, '08701872': 2645, '087018728': 2646, '0870187287': 2647, '08701872873': 2648, '087018728737': 2649, '087018728737,': 2650, '08702': 2651, '087024': 2652, '0870241': 2653, '08702411': 2654, '087024118': 2655, '0870241182': 2656, '08702411827': 2657, '087024118271': 2658, '0870241182716': 2659, '0870249': 2660, '08702490': 2661, '087024900': 2662, '0870249008': 2663, '08702490080': 2664, '087028': 2665, '0870284': 2666, '08702840': 2667, '087028406': 2668, '0870284062': 2669, '08702840625': 2670, '08702840625.': 2671, '08702840625.c': 2672, '08704': 2673, '087044': 2674, '0870443': 2675, '08704439': 2676, '087044396': 2677, '0870443968': 2678, '08704439680': 2679, '08704439680.': 2680, '08704439680ts&cs': 2681, '08706': 2682, '087060': 2683, '0870609': 2684, '08706091': 2685, '087060917': 2686, '0870609179': 2687, '08706091795': 2688, '08707': 2689, '087073': 2690, '0870737': 2691, '08707379': 2692, '087073791': 2693, '0870737910': 2694, '08707379102': 2695, '087073791021': 2696, '0870737910216': 2697, '0870737910216y': 2698, '0870737910216yr': 2699, '0870737910216yrs': 2700, '087075': 2701, '0870750': 2702, '08707500': 2703, '087075000': 2704, '0870750002': 2705, '08707500020': 2706, '08707509': 2707, '087075090': 2708, '0870750902': 2709, '08707509020': 2710, '0870753': 2711, '08707533': 2712, '087075333': 2713, '0870753331': 2714, '08707533310': 2715, '0870753331018+': 2716, '087078': 2717, '0870780': 2718, '08707808': 2719, '087078082': 2720, '0870780822': 2721, '08707808226': 2722, '08707808226.': 2723, '08708': 2724, '087080': 2725, '0870803': 2726, '08708034': 2727, '087080344': 2728, '0870803441': 2729, '08708034412': 2730, '087088': 2731, '0870880': 2732, '08708800': 2733, '087088002': 2734, '0870880028': 2735, '08708800282': 2736, '08709': 2737, '087092': 2738, '0870922': 2739, '08709222': 2740, '087092229': 2741, '0870922292': 2742, '08709222922': 2743, '08709222922.': 2744, '087095': 2745, '0870950': 2746, '08709501': 2747, '087095015': 2748, '0870950152': 2749, '08709501522': 2750, '0871': 2751, '0871-': 2752, '0871-4': 2753, '0871-47': 2754, '0871-471': 2755, '0871-4719': 2756, '0871-4719-': 2757, '0871-4719-5': 2758, '0871-4719-52': 2759, '0871-4719-523': 2760, '0871-8': 2761, '0871-87': 2762, '0871-872': 2763, '0871-872-': 2764, '0871-872-9': 2765, '0871-872-97': 2766, '0871-872-975': 2767, '0871-872-9755': 2768, '0871-872-9758': 2769, '08710': 2770, '087104': 2771, '0871047': 2772, '08710471': 2773, '087104711': 2774, '0871047111': 2775, '08710471114': 2776, '087104711148': 2777, '08712': 2778, '087121': 2779, '0871210': 2780, '08712101': 2781, '087121013': 2782, '0871210135': 2783, '08712101358': 2784, '08712103': 2785, '087121037': 2786, '0871210373': 2787, '08712103738': 2788, '0871212': 2789, '08712120': 2790, '087121202': 2791, '0871212025': 2792, '08712120250': 2793, '087121202501': 2794, '0871212025016': 2795, '087123': 2796, '0871230': 2797, '08712300': 2798, '087123002': 2799, '0871230022': 2800, '08712300220': 2801, '087123002209': 2802, '087123002209am-7pm': 2803, '087123002209am-7pm.': 2804, '0871231': 2805, '08712317': 2806, '087123176': 2807, '0871231760': 2808, '08712317606': 2809, '087124': 2810, '0871240': 2811, '08712400': 2812, '087124002': 2813, '0871240020': 2814, '08712400200': 2815, '08712400200.': 2816, '087124006': 2817, '0871240060': 2818, '08712400603': 2819, '08712402': 2820, '087124020': 2821, '0871240205': 2822, '08712402050': 2823, '087124025': 2824, '0871240257': 2825, '08712402578': 2826, '087124027': 2827, '0871240277': 2828, '08712402779': 2829, '087124029': 2830, '0871240290': 2831, '08712402902': 2832, '0871240297': 2833, '08712402972': 2834, '08712404': 2835, '087124040': 2836, '0871240400': 2837, '08712404000': 2838, '08712405': 2839, '087124050': 2840, '0871240502': 2841, '08712405020': 2842, '08712405020.': 2843, '08712405022': 2844, '08712405022,': 2845, '0871246': 2846, '08712460': 2847, '087124603': 2848, '0871246032': 2849, '08712460324': 2850, '08712460324(': 2851, '08712460324(n': 2852, '08712460324(na': 2853, '08712466': 2854, '087124666': 2855, '0871246666': 2856, '08712466669': 2857, '087127': 2858, '0871277': 2859, '08712778': 2860, '087127781': 2861, '0871277810': 2862, '08712778107': 2863, '087127781071': 2864, '0871277810710': 2865, '0871277810710p': 2866, '0871277810710p/': 2867, '0871277810710p/m': 2868, '0871277810710p/mi': 2869, '0871277810710p/min': 2870, '08712778108': 2871, '087127781081': 2872, '0871277810810': 2873, '08712778109': 2874, '087127781091': 2875, '0871277810910': 2876, '0871277810910p': 2877, '0871277810910p/': 2878, '0871277810910p/m': 2879, '0871277810910p/mi': 2880, '0871277810910p/min': 2881, '08714': 2882, '087143': 2883, '0871434': 2884, '08714342': 2885, '087143423': 2886, '0871434239': 2887, '08714342399': 2888, '08714342399.': 2889, '08714342399.2': 2890, '08714342399.2s': 2891, '087147': 2892, '0871471': 2893, '08714712': 2894, '087147123': 2895, '0871471237': 2896, '08714712377': 2897, '087147123779': 2898, '087147123779am-7pm': 2899, '087147123779am-7pm.': 2900, '08714712379': 2901, '0871471238': 2902, '08714712388': 2903, '0871471239': 2904, '08714712394': 2905, '087147124': 2906, '0871471241': 2907, '08714712412': 2908, '08714714': 2909, '087147140': 2910, '0871471401': 2911, '08714714011': 2912, '08715': 2913, '087152': 2914, '0871520': 2915, '08715203': 2916, '087152030': 2917, '0871520302': 2918, '08715203028': 2919, '087152036': 2920, '0871520364': 2921, '08715203649': 2922, '0871520365': 2923, '08715203652': 2924, '08715203656': 2925, '0871520367': 2926, '08715203677': 2927, '0871520368': 2928, '08715203685': 2929, '0871520369': 2930, '08715203694': 2931, '08715205': 2932, '087152052': 2933, '0871520527': 2934, '08715205273': 2935, '087155': 2936, '0871550': 2937, '08715500': 2938, '087155000': 2939, '0871550002': 2940, '08715500022': 2941, '087157': 2942, '0871570': 2943, '08715705': 2944, '087157050': 2945, '0871570502': 2946, '08715705022': 2947, '08715705022,': 2948, '08717': 2949, '087171': 2950, '0871711': 2951, '08717111': 2952, '087171118': 2953, '0871711182': 2954, '08717111821': 2955, '0871716': 2956, '08717168': 2957, '087171685': 2958, '0871716852': 2959, '08717168528': 2960, '087172': 2961, '0871720': 2962, '08717205': 2963, '087172055': 2964, '0871720554': 2965, '08717205546': 2966, '087175': 2967, '0871750': 2968, '0871750.': 2969, '0871750.77': 2970, '0871750.77.': 2971, '0871750.77.11!': 2972, '08717507': 2973, '087175073': 2974, '0871750738': 2975, '08717507382': 2976, '08717509': 2977, '087175099': 2978, '0871750999': 2979, '08717509990': 2980, '087178': 2981, '0871789': 2982, '08717890': 2983, '087178908': 2984, '0871789089': 2985, '08717890890': 2986, '08717890890å£1': 2987, '08717890890å£1.50': 2988, '08717895': 2989, '087178956': 2990, '0871789569': 2991, '08717895698': 2992, '08717898': 2993, '087178980': 2994, '0871789803': 2995, '08717898035': 2996, '08717898035.': 2997, '08718': 2998, '087187': 2999, '0871871': 3000, '08718711': 3001, '087187111': 3002, '0871871110': 3003, '08718711108': 3004, '0871872': 3005, '08718720': 3006, '087187202': 3007, '0871872020': 3008, '08718720201': 3009, '08718723': 3010, '087187238': 3011, '0871872381': 3012, '08718723815': 3013, '08718723815.': 3014, '08718725': 3015, '087187257': 3016, '0871872575': 3017, '08718725756': 3018, '08718725756.': 3019, '08718726': 3020, '087187262': 3021, '0871872627': 3022, '08718726270': 3023, '087187262701': 3024, '087187262701.50': 3025, '087187262701.50g': 3026, '087187262701.50gb': 3027, '087187262701.50gbp': 3028, '087187262701.50gbp/': 3029, '087187262701.50gbp/m': 3030, '087187262701.50gbp/mt': 3031, '087187262701.50gbp/mtmsg18': 3032, '087187269': 3033, '0871872697': 3034, '08718726970': 3035, '08718726971': 3036, '08718726978': 3037, '08718727': 3038, '087187272': 3039, '0871872720': 3040, '08718727200': 3041, '087187272008': 3042, '087187278': 3043, '0871872786': 3044, '08718727868': 3045, '08718727868.': 3046, '0871872787': 3047, '08718727870': 3048, '0871873': 3049, '08718730': 3050, '087187305': 3051, '0871873055': 3052, '08718730555': 3053, '087187306': 3054, '0871873066': 3055, '08718730666': 3056, '08718738': 3057, '087187380': 3058, '0871873800': 3059, '08718738001': 3060, '08718738002': 3061, '0871873803': 3062, '08718738034': 3063, '08718738034.': 3064, '08719': 3065, '087191': 3066, '0871918': 3067, '08719180': 3068, '087191802': 3069, '0871918021': 3070, '08719180219': 3071, '0871918024': 3072, '08719180248': 3073, '08719181': 3074, '087191812': 3075, '0871918125': 3076, '08719181259': 3077, '087191815': 3078, '0871918150': 3079, '08719181503': 3080, '0871918151': 3081, '08719181513': 3082, '08719181513.': 3083, '087198': 3084, '0871983': 3085, '08719839': 3086, '087198398': 3087, '0871983983': 3088, '08719839835': 3089, '08719839835.': 3090, '0871989': 3091, '08719899': 3092, '087198992': 3093, '0871989921': 3094, '08719899217': 3095, '0871989922': 3096, '08719899229': 3097, '0871989923': 3098, '08719899230': 3099, '09': 3100, '090': 3101, '0904': 3102, '09041': 3103, '090419': 3104, '0904194': 3105, '09041940': 3106, '090419402': 3107, '0904194022': 3108, '09041940223': 3109, '0905': 3110, '09050': 3111, '090500': 3112, '0905000': 3113, '09050000': 3114, '090500003': 3115, '0905000030': 3116, '09050000301': 3117, '09050000301.': 3118, '0905000033': 3119, '09050000332': 3120, '090500004': 3121, '0905000046': 3122, '09050000460': 3123, '090500005': 3124, '0905000055': 3125, '09050000555': 3126, '09050000555.': 3127, '09050000878': 3128, '09050000878.': 3129, '090500009': 3130, '0905000092': 3131, '09050000928': 3132, '09050000928.': 3133, '09050001': 3134, '090500012': 3135, '0905000129': 3136, '09050001295': 3137, '090500018': 3138, '0905000180': 3139, '09050001808': 3140, '09050002': 3141, '090500023': 3142, '0905000231': 3143, '09050002311': 3144, '09050003': 3145, '090500030': 3146, '0905000309': 3147, '09050003091': 3148, '09050005': 3149, '090500053': 3150, '0905000532': 3151, '09050005321': 3152, '09050005321.': 3153, '0905009': 3154, '09050090': 3155, '090500900': 3156, '0905009004': 3157, '09050090044': 3158, '090502': 3159, '0905028': 3160, '09050280': 3161, '090502805': 3162, '0905028052': 3163, '09050280520': 3164, '09050280520,': 3165, '09053': 3166, '090537': 3167, '0905375': 3168, '09053750': 3169, '090537500': 3170, '0905375000': 3171, '09053750005': 3172, '09056': 3173, '090562': 3174, '0905624': 3175, '09056242': 3176, '090562421': 3177, '0905624215': 3178, '09056242159': 3179, '09057': 3180, '090570': 3181, '0905703': 3182, '09057039': 3183, '090570399': 3184, '0905703999': 3185, '09057039994': 3186, '09058': 3187, '090580': 3188, '0905809': 3189, '09058091': 3190, '090580918': 3191, '0905809185': 3192, '09058091854': 3193, '0905809187': 3194, '09058091870': 3195, '09058094': 3196, '090580944': 3197, '0905809445': 3198, '09058094454': 3199, '09058094455': 3200, '090580945': 3201, '0905809450': 3202, '09058094507': 3203, '0905809456': 3204, '09058094565': 3205, '0905809458': 3206, '09058094583': 3207, '0905809459': 3208, '09058094594': 3209, '09058094597': 3210, '09058094599': 3211, '09058095': 3212, '090580951': 3213, '0905809510': 3214, '09058095107': 3215, '090580952': 3216, '0905809520': 3217, '09058095201': 3218, '09058097': 3219, '090580971': 3220, '0905809718': 3221, '09058097189': 3222, '090580972': 3223, '0905809721': 3224, '09058097218': 3225, '09058098': 3226, '090580980': 3227, '0905809800': 3228, '09058098002': 3229, '09058098002.': 3230, '09058099': 3231, '090580998': 3232, '0905809980': 3233, '09058099801': 3234, '0906': 3235, '09061': 3236, '090611': 3237, '0906110': 3238, '09061104': 3239, '090611042': 3240, '0906110427': 3241, '09061104276': 3242, '0906110428': 3243, '09061104283': 3244, '090612': 3245, '0906120': 3246, '09061209': 3247, '090612094': 3248, '0906120946': 3249, '09061209465': 3250, '0906121': 3251, '09061213': 3252, '090612132': 3253, '0906121323': 3254, '09061213237': 3255, '0906122': 3256, '09061221': 3257, '090612210': 3258, '0906122106': 3259, '09061221061': 3260, '09061221066': 3261, '090617': 3262, '0906170': 3263, '09061701': 3264, '090617014': 3265, '0906170144': 3266, '09061701444': 3267, '09061701444.': 3268, '0906170146': 3269, '09061701461': 3270, '09061701461.': 3271, '090617018': 3272, '0906170185': 3273, '09061701851': 3274, '09061701851.': 3275, '090617019': 3276, '0906170193': 3277, '09061701939': 3278, '09061701939.': 3279, '09061702': 3280, '090617028': 3281, '0906170289': 3282, '09061702893': 3283, '0906174': 3284, '09061743': 3285, '090617433': 3286, '0906174338': 3287, '09061743386': 3288, '090617438': 3289, '0906174380': 3290, '09061743806': 3291, '0906174381': 3292, '09061743810': 3293, '09061743811': 3294, '09061744': 3295, '090617445': 3296, '0906174455': 3297, '09061744553': 3298, '09061749': 3299, '090617496': 3300, '0906174960': 3301, '09061749602': 3302, '0906179': 3303, '09061790': 3304, '090617901': 3305, '0906179012': 3306, '09061790121': 3307, '09061790125': 3308, '09061790126': 3309, '09063': 3310, '090634': 3311, '0906344': 3312, '09063440': 3313, '090634404': 3314, '0906344045': 3315, '09063440451': 3316, '09063442': 3317, '090634421': 3318, '0906344215': 3319, '09063442151': 3320, '0906345': 3321, '09063458': 3322, '090634581': 3323, '0906345813': 3324, '09063458130': 3325, '0906346': 3326, '09063463': 3327, '090634633': 3328, '0906346330': 3329, '0906346330.': 3330, '09064': 3331, '090640': 3332, '0906401': 3333, '09064011': 3334, '090640110': 3335, '0906401100': 3336, '09064011000': 3337, '09064011000.': 3338, '09064012': 3339, '090640121': 3340, '0906401210': 3341, '09064012103': 3342, '0906401216': 3343, '09064012160': 3344, '09064012160.': 3345, '09064015': 3346, '090640153': 3347, '0906401530': 3348, '09064015307': 3349, '09064017': 3350, '090640172': 3351, '0906401729': 3352, '09064017295': 3353, '090640173': 3354, '0906401730': 3355, '09064017305': 3356, '09064018': 3357, '090640188': 3358, '0906401883': 3359, '09064018838': 3360, '09064018838.': 3361, '09064019': 3362, '090640190': 3363, '0906401901': 3364, '09064019014': 3365, '090640197': 3366, '0906401978': 3367, '09064019788': 3368, '09065': 3369, '090650': 3370, '0906506': 3371, '09065069': 3372, '090650691': 3373, '0906506912': 3374, '09065069120': 3375, '0906506915': 3376, '09065069154': 3377, '090651': 3378, '0906517': 3379, '09065171': 3380, '090651711': 3381, '0906517114': 3382, '09065171142': 3383, '09065171142-': 3384, '09065171142-s': 3385, '09065171142-st': 3386, '09065174': 3387, '090651740': 3388, '0906517404': 3389, '09065174042': 3390, '09065174042.': 3391, '090653': 3392, '0906539': 3393, '09065394': 3394, '090653945': 3395, '0906539451': 3396, '09065394514': 3397, '090653949': 3398, '0906539497': 3399, '09065394973': 3400, '090659': 3401, '0906598': 3402, '09065989': 3403, '090659891': 3404, '0906598918': 3405, '09065989180': 3406, '09065989182': 3407, '09066': 3408, '090663': 3409, '0906635': 3410, '09066350': 3411, '090663507': 3412, '0906635075': 3413, '09066350750': 3414, '09066358': 3415, '090663581': 3416, '0906635815': 3417, '09066358152': 3418, '090663583': 3419, '0906635836': 3420, '09066358361': 3421, '0906636': 3422, '09066361': 3423, '090663619': 3424, '0906636192': 3425, '09066361921': 3426, '09066362': 3427, '090663622': 3428, '0906636220': 3429, '09066362206': 3430, '0906636222': 3431, '09066362220': 3432, '0906636223': 3433, '09066362231': 3434, '09066364': 3435, '090663643': 3436, '0906636431': 3437, '09066364311': 3438, '0906636434': 3439, '09066364349': 3440, '090663645': 3441, '0906636458': 3442, '09066364589': 3443, '09066368': 3444, '090663683': 3445, '0906636832': 3446, '09066368327': 3447, '090663684': 3448, '0906636847': 3449, '09066368470': 3450, '090663687': 3451, '0906636875': 3452, '09066368753': 3453, '0906638': 3454, '09066380': 3455, '090663806': 3456, '0906638061': 3457, '09066380611': 3458, '09066382': 3459, '090663824': 3460, '0906638242': 3461, '09066382422': 3462, '090666': 3463, '0906661': 3464, '09066612': 3465, '090666126': 3466, '0906661266': 3467, '09066612661': 3468, '0906664': 3469, '09066649': 3470, '090666497': 3471, '0906664973': 3472, '09066649731': 3473, '09066649731f': 3474, '09066649731fr': 3475, '0906666': 3476, '09066660': 3477, '090666601': 3478, '0906666010': 3479, '09066660100': 3480, '0907': 3481, '09071': 3482, '090715': 3483, '0907151': 3484, '09071512': 3485, '090715124': 3486, '0907151243': 3487, '09071512432': 3488, '09071512433': 3489, '09071517': 3490, '090715178': 3491, '0907151786': 3492, '09071517866': 3493, '09077': 3494, '090778': 3495, '0907781': 3496, '09077818': 3497, '090778181': 3498, '0907781815': 3499, '09077818151': 3500, '0909': 3501, '09090': 3502, '090902': 3503, '0909020': 3504, '09090204': 3505, '090902044': 3506, '0909020444': 3507, '09090204448': 3508, '090909': 3509, '0909090': 3510, '09090900': 3511, '090909000': 3512, '0909090004': 3513, '09090900040': 3514, '09094': 3515, '090941': 3516, '0909410': 3517, '09094100': 3518, '090941001': 3519, '0909410015': 3520, '09094100151': 3521, '090946': 3522, '0909464': 3523, '09094646': 3524, '090946466': 3525, '0909464663': 3526, '09094646631': 3527, '090946468': 3528, '0909464689': 3529, '09094646899': 3530, '09095': 3531, '090953': 3532, '0909535': 3533, '09095350': 3534, '090953503': 3535, '0909535030': 3536, '09095350301': 3537, '09096': 3538, '090961': 3539, '0909610': 3540, '09096102': 3541, '090961023': 3542, '0909610231': 3543, '09096102316': 3544, '09099': 3545, '090997': 3546, '0909972': 3547, '09099725': 3548, '090997258': 3549, '0909972582': 3550, '09099725823': 3551, '09099726': 3552, '090997263': 3553, '0909972639': 3554, '09099726395': 3555, '090997264': 3556, '0909972642': 3557, '09099726429': 3558, '0909972648': 3559, '09099726481': 3560, '090997265': 3561, '0909972655': 3562, '09099726553': 3563, '091': 3564, '0911': 3565, '09111': 3566, '091110': 3567, '0911103': 3568, '09111030': 3569, '091110301': 3570, '0911103011': 3571, '09111030116': 3572, '09111030116.': 3573, '09111032': 3574, '091110321': 3575, '0911103212': 3576, '09111032124': 3577, '097': 3578, '0970': 3579, '09701': 3580, '097012': 3581, '0970121': 3582, '09701213': 3583, '097012131': 3584, '0970121318': 3585, '09701213186': 3586, '0a$': 3587, '0a$n': 3588, '1': 3589, '1)': 3590, '1-': 3591, '1-h': 3592, '1-ha': 3593, '1-han': 3594, '1-hanu': 3595, '1-hanum': 3596, '1-hanuma': 3597, '1-hanuman': 3598, '1-m': 3599, '1-u': 3600, '1.20': 3601, '1.50': 3602, '1.50p': 3603, '1.50p/': 3604, '1.50p/w': 3605, '1.50p/wk': 3606, '1.50p/wk.': 3607, '1.5p/m': 3608, '1.5p/mi': 3609, '1.5p/min': 3610, '1/08': 3611, '1/08/': 3612, '1/08/03': 3613, '1/08/03!': 3614, '1/1': 3615, '1/2': 3616, '1/3': 3617, '1/3.': 3618, '10': 3619, '10,000': 3620, '10/06/': 3621, '10/06/0': 3622, '10/06/03': 3623, '10/06/03!': 3624, '100': 3625, '1000': 3626, \"1000'\": 3627, \"1000's\": 3628, '1013': 3629, '1030': 3630, '10:10': 3631, '10:30': 3632, '10am-7pm': 3633, '10am-9pm': 3634, '10p': 3635, '10p/': 3636, '10p/m': 3637, '10p/mi': 3638, '10p/min': 3639, '11': 3640, '11.48': 3641, '1120': 3642, '113': 3643, '113,': 3644, '1131': 3645, '114': 3646, '114/': 3647, '114/14': 3648, '116': 3649, '1172': 3650, '1172.': 3651, '118': 3652, '118p': 3653, '118p/': 3654, '118p/m': 3655, '118p/ms': 3656, '118p/msg': 3657, '11?': 3658, '11m': 3659, '11mt': 3660, '11mth': 3661, '11mths': 3662, '11mths+?': 3663, '12': 3664, '12,000': 3665, '12,000p': 3666, '1205': 3667, '121': 3668, '1225': 3669, '123': 3670, '125': 3671, '125g': 3672, '125gi': 3673, '125gif': 3674, '125gift': 3675, '128': 3676, '12:30': 3677, '12h': 3678, '12hr': 3679, '12hrs': 3680, '13/10/04': 3681, '13/4/04': 3682, '13/4/04.': 3683, '130': 3684, '130.': 3685, '1327': 3686, '139': 3687, '139,': 3688, '14': 3689, '140': 3690, '1405': 3691, '1405,': 3692, '140p': 3693, '140pp': 3694, '140ppm': 3695, '145': 3696, '146': 3697, '146t': 3698, '146tf150p': 3699, '14t': 3700, '14th': 3701, '14thm': 3702, '14thma': 3703, '14thmar': 3704, '14thmarc': 3705, '14thmarch': 3706, '14thmarch.': 3707, '15': 3708, '150': 3709, '150p': 3710, '150p/': 3711, '150p/m': 3712, '150p/ms': 3713, '150p/msg': 3714, '150p/mtmsgrcvd18': 3715, '150p/mtmsgrcvd18+': 3716, '150pp': 3717, '150ppm': 3718, '151': 3719, '151.': 3720, '153': 3721, '155': 3722, '1554': 3723, '15541': 3724, '15:': 3725, '15:2': 3726, '15:26': 3727, '15:26,': 3728, '16': 3729, '16+': 3730, '1680': 3731, '1680,': 3732, '169': 3733, '177': 3734, '18': 3735, '18+': 3736, '18+6*å£1': 3737, '18+6*å£1.': 3738, '18+6*å£1.50(m': 3739, '18/11/04': 3740, '184': 3741, '1843': 3742, '1843.': 3743, '195': 3744, '1956669': 3745, '1x150p/': 3746, '1x150p/w': 3747, '1x150p/wk': 3748, '2': 3749, '2%': 3750, '2)': 3751, '2-4-1': 3752, '2.15': 3753, '2.30': 3754, '2.30i': 3755, '2.30is': 3756, '2.30ish': 3757, '2.50': 3758, '2/2': 3759, '20': 3760, '20%': 3761, '20,000': 3762, '200': 3763, '2000': 3764, '2003': 3765, '2004': 3766, '2004,': 3767, '2005': 3768, '2005.': 3769, '2006': 3770, '2007': 3771, '20250': 3772, '2025050': 3773, '20m12': 3774, '20m12a': 3775, '20m12aq': 3776, '20m12aq.': 3777, '20p': 3778, '20p/': 3779, '20p/m': 3780, '20p/mi': 3781, '20p/min': 3782, '21': 3783, '21/': 3784, '21/11/04': 3785, '21/m': 3786, '21870': 3787, '218700': 3788, '2187000': 3789, '21870000': 3790, '21870000>': 3791, '21870000>h': 3792, '21870000>hi': 3793, '220-': 3794, '220-c': 3795, '220-cm': 3796, '220-cm2': 3797, '2309': 3798, '2309.': 3799, '24': 3800, '24/10/04': 3801, '24/7': 3802, '24/7m': 3803, '24/7mp': 3804, '25': 3805, '250': 3806, '255': 3807, '255.': 3808, '25p': 3809, '26': 3810, '26.03': 3811, '26.03.': 3812, '26.03.05': 3813, '26/10/04': 3814, '26/11/04': 3815, '2667': 3816, '26t': 3817, '26th': 3818, '27/03': 3819, '27/6/03': 3820, '27/6/03.': 3821, '28': 3822, '28,': 3823, '28/5': 3824, '2814032': 3825, '29/03/05': 3826, '29/10/0': 3827, '3': 3828, '3)': 3829, '3)c': 3830, '3)u': 3831, '3)un': 3832, '3)unk': 3833, '3-': 3834, '3-m': 3835, '3-ma': 3836, '3-mar': 3837, '3-maru': 3838, '3-marut': 3839, '3-maruti': 3840, '3-u': 3841, '30': 3842, '300': 3843, '300%': 3844, '3006': 3845, '30060': 3846, '300603': 3847, '300p': 3848, '3030': 3849, '3030.': 3850, '30p': 3851, '30pp': 3852, '30pp/': 3853, '30pp/t': 3854, '31': 3855, '31/10/04': 3856, '3100': 3857, '310303': 3858, '310303.': 3859, '31p': 3860, '31p.': 3861, '31p.m': 3862, '31p.ms': 3863, '31p.msg@150p': 3864, '32000': 3865, '3230': 3866, '32323': 3867, '32323.': 3868, '326': 3869, '330': 3870, '330.': 3871, '35': 3872, '351': 3873, '3510': 3874, '3510i': 3875, '35p': 3876, '35p.': 3877, '36': 3878, '3650': 3879, '36504': 3880, '368': 3881, '3680': 3882, '373': 3883, '3750': 3884, '391784': 3885, '391784.': 3886, '3qxj9': 3887, '3xx': 3888, '3xå£150': 3889, '3xå£150p': 3890, '3xå£150pw': 3891, '4': 3892, '4)': 3893, '4)p': 3894, '4)pr': 3895, '4*': 3896, '4-6': 3897, '4-6.': 3898, '4-6..': 3899, '4-6...': 3900, '4-7/12': 3901, '4-7/12.': 3902, '4.15': 3903, '4.30': 3904, '4.47p': 3905, '4.49/m': 3906, '4.50': 3907, '4.50.': 3908, '40': 3909, '400': 3910, '400m': 3911, '400mi': 3912, '400min': 3913, '400mins': 3914, '400mins.': 3915, '400mins..': 3916, '400mins...': 3917, '400mins...c': 3918, '400mins...ca': 3919, '400mins...cal': 3920, '400mins...call': 3921, '402': 3922, '402.': 3923, '4041': 3924, '40411': 3925, '40533': 3926, '40g': 3927, '40gb': 3928, '40m': 3929, '40mp': 3930, '40mph': 3931, '40mph.': 3932, '41685': 3933, '41782': 3934, '420': 3935, '42049': 3936, '4217': 3937, '42478': 3938, '42810': 3939, '430': 3940, '434': 3941, '44': 3942, '4403': 3943, '4403l': 3944, '4403ld': 3945, '4403ldn': 3946, '4403ldnw1a7rw18': 3947, '4478': 3948, '44780': 3949, '447801': 3950, '4478012': 3951, '44780125': 3952, '447801259': 3953, '4478012592': 3954, '44780125923': 3955, '447801259231': 3956, '4487124': 3957, '44871240': 3958, '448712404': 3959, '4487124040': 3960, '44871240400': 3961, '448712404000': 3962, '448712404000>': 3963, '448712404000>p': 3964, '448712404000>pl': 3965, '449': 3966, '4490': 3967, '449050': 3968, '4490500': 3969, '44905000': 3970, '449050000': 3971, '4490500003': 3972, '44905000030': 3973, '449050000301': 3974, '45': 3975, '450': 3976, '450p': 3977, '450pw': 3978, '45239': 3979, '47': 3980, '474': 3981, '4742': 3982, '4882': 3983, '48922': 3984, '49557': 3985, '4t&c': 3986, '4t&ct': 3987, '4t&ctx': 3988, '4t&ctxt': 3989, '4txt/120p': 3990, '4txt/ì¼1.20': 3991, '5': 3992, '5)': 3993, '5)g': 3994, '5+': 3995, '5+-': 3996, '5+3': 3997, '5+3+': 3998, '5+3+2': 3999, '5+3+2=': 4000, '5-': 4001, '5-s': 4002, '5-sa': 4003, '5-san': 4004, '5-sank': 4005, '5-sanka': 4006, '5-sankat': 4007, '5-sankatm': 4008, '5.15': 4009, '5.15p': 4010, '5.15pm': 4011, '5.30': 4012, '5/9/03': 4013, '50': 4014, '50%': 4015, '500': 4016, '5000': 4017, '50506': 4018, '505060': 4019, '50\\\\\"\"': 4020, '50p': 4021, '515': 4022, '5226': 4023, '526': 4024, '526,': 4025, '528': 4026, '530': 4027, '54': 4028, '542': 4029, '545': 4030, '5w': 4031, '5wb': 4032, '5wq': 4033, '6': 4034, '6-': 4035, '6-r': 4036, '6-ra': 4037, '6-ram': 4038, '6-rama': 4039, '6-ramad': 4040, '6-ramadu': 4041, '6-ramadut': 4042, '6-ramaduth': 4043, '6.30': 4044, '6.45pm': 4045, '6.45pm.': 4046, '60': 4047, '60,400': 4048, '600': 4049, '603': 4050, '6031': 4051, '60p': 4052, '60p/': 4053, '60p/m': 4054, '60p/mi': 4055, '60p/min': 4056, '60p/min.': 4057, '61200': 4058, '616': 4059, '6161': 4060, '61610': 4061, '62220': 4062, '62220c': 4063, '62220cn': 4064, '62220cnc': 4065, '62220cncl': 4066, '6230': 4067, '62468': 4068, '62735=å£45': 4069, '62735=å£450': 4070, '63': 4071, '630': 4072, '63m': 4073, '63mi': 4074, '63mil': 4075, '645': 4076, '650': 4077, '650.': 4078, '666': 4079, '6669': 4080, '674': 4081, '6744': 4082, '67441': 4083, '674412': 4084, '6744123': 4085, '67441233': 4086, '68866': 4087, '68866.': 4088, '69': 4089, '691': 4090, '6910': 4091, '69101': 4092, '69101.': 4093, '692': 4094, '6920': 4095, '69200': 4096, '696': 4097, '6966': 4098, '69669': 4099, '6969': 4100, '69696': 4101, '69698': 4102, '698': 4103, '6985': 4104, '69855': 4105, '69855,': 4106, '6986': 4107, '69866': 4108, '69866.': 4109, '69866.18': 4110, '6987': 4111, '69876': 4112, '69876.': 4113, '6988': 4114, '69888': 4115, '69888!': 4116, '699': 4117, '69911(å£': 4118, '69911(å£1': 4119, '69911(å£1.50': 4120, '69911(å£1.50p': 4121, '69911(å£1.50p.': 4122, '6996': 4123, '69969': 4124, '6998': 4125, '69988': 4126, '7': 4127, '7+': 4128, '7+2': 4129, '7+2+': 4130, '7+2+5': 4131, '7+2+5=': 4132, '7+2+5=?': 4133, '7+2+5=??': 4134, '7+2+5=???': 4135, '7+2+5=????': 4136, '7+2+5=?????': 4137, '7-': 4138, '7-m': 4139, '7-ma': 4140, '7-mah': 4141, '7-maha': 4142, '7-mahav': 4143, '7.30': 4144, '7.30p': 4145, '7.30pm': 4146, '7250': 4147, '730': 4148, '730.': 4149, '731': 4150, '74355': 4151, '75': 4152, '750': 4153, '754': 4154, '7548': 4155, '76': 4156, '7634': 4157, '768': 4158, '7684': 4159, '773': 4160, '7732': 4161, '77325': 4162, '773258': 4163, '7732584': 4164, '77325843': 4165, '773258435': 4166, '7732584351': 4167, '7732584351,': 4168, '78': 4169, '786': 4170, '787': 4171, '7876': 4172, '78761': 4173, '787615': 4174, '7876150': 4175, '7876150p': 4176, '7876150pp': 4177, '7876150ppm': 4178, '7:30': 4179, '7z': 4180, '7zs': 4181, '8': 4182, '8+6+': 4183, '8+6+3': 4184, '8+6+3=': 4185, '8-': 4186, '8-8:30': 4187, '8-h': 4188, '8-hr': 4189, '8.30': 4190, '80': 4191, \"80'\": 4192, \"80's\": 4193, '800': 4194, '80009307': 4195, '800093070': 4196, '8000930705': 4197, '80062': 4198, '8007': 4199, '8008': 4200, '80082': 4201, '80086': 4202, '801': 4203, '8012': 4204, '80122': 4205, '801223': 4206, '80122300': 4207, '80122300p': 4208, '80122300p/': 4209, '80122300p/w': 4210, '80122300p/wk': 4211, '80155': 4212, '80155,': 4213, '8016': 4214, '80160': 4215, '8018': 4216, '80182': 4217, '8027': 4218, '804': 4219, '8048': 4220, '80488': 4221, '806': 4222, '8060': 4223, '80608': 4224, '80608.': 4225, '8077': 4226, '80878': 4227, '80878.': 4228, '81010': 4229, '81151': 4230, '81303': 4231, '816': 4232, '8161': 4233, '81618': 4234, '81618,': 4235, '81618-': 4236, '81618-?': 4237, '81618-?3': 4238, '82242': 4239, '82277': 4240, '82277.': 4241, '823': 4242, '8232': 4243, '82324': 4244, '82324.': 4245, '82468': 4246, '83': 4247, '830': 4248, '8302': 4249, '83021': 4250, '83021.': 4251, '83039': 4252, '83049': 4253, '83049.': 4254, '831': 4255, '8311': 4256, '83110': 4257, '83118': 4258, '832': 4259, '8322': 4260, '83222': 4261, '83222.': 4262, '833': 4263, '8333': 4264, '83332': 4265, '83332.': 4266, '83332.p': 4267, '83332.pl': 4268, '83338': 4269, '83355': 4270, '83355!': 4271, '8337': 4272, '83370': 4273, '83370.': 4274, '8338': 4275, '83383': 4276, '834': 4277, '8343': 4278, '83435': 4279, '83435.': 4280, '836': 4281, '8360': 4282, '83600': 4283, '8373': 4284, '83738': 4285, '83738.': 4286, '84': 4287, '84,': 4288, '840': 4289, '84025': 4290, '841': 4291, '8412': 4292, '84122': 4293, '84128': 4294, '84128,': 4295, '84128,c': 4296, '84128,cu': 4297, '84128,cus': 4298, '84199': 4299, '844': 4300, '8448': 4301, '84484': 4302, '85': 4303, '850': 4304, '850.': 4305, '8502': 4306, '85023': 4307, '85069': 4308, '852': 4309, '8522': 4310, '85222': 4311, '8523': 4312, '85233': 4313, '855': 4314, '8552': 4315, '85555': 4316, '86': 4317, '860': 4318, '8602': 4319, '86021': 4320, '861': 4321, '864': 4322, '8642': 4323, '86423': 4324, '864233': 4325, '864233.': 4326, '866': 4327, '8668': 4328, '86688': 4329, '868': 4330, '8688': 4331, '86888': 4332, '87': 4333, '870': 4334, '8702': 4335, '87021': 4336, '87021.': 4337, '87066': 4338, '8707': 4339, '87070': 4340, '87070.': 4341, '87077': 4342, '87077:': 4343, '871': 4344, '8712': 4345, '87121': 4346, '8713': 4347, '87131': 4348, '8714': 4349, '87147': 4350, '871471': 4351, '8714714': 4352, '87239': 4353, '875': 4354, '8757': 4355, '87575': 4356, '87575.': 4357, '88': 4358, '880': 4359, '8800': 4360, '8800,': 4361, '88039': 4362, '88066': 4363, '8808': 4364, '88088': 4365, '882': 4366, '8822': 4367, '88222': 4368, '886': 4369, '8860': 4370, '88600': 4371, '888': 4372, '88800': 4373, '8883': 4374, '88877': 4375, '88877>': 4376, '88877>f': 4377, '88877>fr': 4378, '8888': 4379, '88888': 4380, '89': 4381, '890': 4382, '8903': 4383, '89034': 4384, '8907': 4385, '89070': 4386, '8908': 4387, '89080': 4388, '891': 4389, '8910': 4390, '89105': 4391, '89105.': 4392, '8912': 4393, '89123': 4394, '89123\"': 4395, '895': 4396, '8954': 4397, '89545': 4398, '8955': 4399, '89555': 4400, '896': 4401, '8969': 4402, '89693': 4403, '899': 4404, '8993': 4405, '89938': 4406, '9': 4407, '9+': 4408, '9+2': 4409, '9+2+': 4410, '9+2+4': 4411, '9+2+4=': 4412, '9-6': 4413, '9-6.': 4414, '906': 4415, '9061': 4416, '90611': 4417, '906110': 4418, '9061100': 4419, '90611000': 4420, '906110001': 4421, '9061100010': 4422, '915': 4423, '9153': 4424, '9153.': 4425, '930': 4426, '945+': 4427, '946': 4428, '97': 4429, '97n': 4430, '97n7qp,': 4431, '9832156': 4432, '98321561': 4433, '999': 4434, '9996': 4435, '9996.': 4436, '9am-11pm': 4437, '9j': 4438, '9ja': 4439, ':': 4440, ':(': 4441, ':)': 4442, ':)\"': 4443, ':-': 4444, ':-(': 4445, ':-)': 4446, ':-):):-):-):-)': 4447, ':-):):-):-):-).': 4448, ':-):-)': 4449, ':-/': 4450, ':-d': 4451, ':-p': 4452, ':-x': 4453, ':-xx': 4454, ':/': 4455, ':v': 4456, ':z': 4457, ';': 4458, ';)': 4459, ';-(': 4460, ';-)': 4461, ';_': 4462, ';_;': 4463, ';a': 4464, ';ab': 4465, ';v': 4466, '<': 4467, '<f': 4468, '<u': 4469, '<ukp>2000': 4470, '=': 4471, '=)': 4472, '=/': 4473, '>': 4474, '>>': 4475, '>>>': 4476, '>>>m': 4477, '?': 4478, '?1,000': 4479, '?2,000': 4480, '?350': 4481, '@': 4482, '[': 4483, '[c': 4484, '[i': 4485, '[in': 4486, '[sic]': 4487, '[\\x89û': 4488, '[\\x89û_': 4489, '[\\x89û_]': 4490, '\\\\': 4491, '\\\\\"': 4492, '\\\\\"\"': 4493, '\\\\\"1': 4494, '\\\\\"1.': 4495, '\\\\\"1.u': 4496, '\\\\3000': 4497, '\\\\j': 4498, '\\\\ju': 4499, '\\\\jul': 4500, '\\\\juli': 4501, '\\\\julia': 4502, '\\\\julian': 4503, '\\\\juliana': 4504, '\\\\julianal': 4505, '\\\\julianala': 4506, '\\\\julianalan': 4507, '\\\\julianaland': 4508, '\\\\julianaland\\\\\"': 4509, '\\\\mix\\\\\"': 4510, '\\\\x\\\\\"': 4511, '^': 4512, '_': 4513, '__': 4514, '___': 4515, '____': 4516, 'a': 4517, 'a21': 4518, 'a21.': 4519, 'a30': 4520, 'a30.': 4521, 'ac/w/icmb3ck': 4522, 'ac/w/icmb3ckt': 4523, 'ac/w/icmb3cktz8r7!-4': 4524, 'acl03530150pm': 4525, 'al!!!!!!!!!': 4526, 'album-qu': 4527, 'album-qui': 4528, 'album-quit': 4529, 'alwa....!!:)': 4530, 'ard...ìä': 4531, 'aå£1.50': 4532, 'b': 4533, 'b4': 4534, 'b4190604,': 4535, 'b4280703': 4536, 'b4280703.': 4537, 'ba128': 4538, 'ba128n': 4539, 'ba128nn': 4540, 'ba128nnfwfly150ppm': 4541, 'basq!ih': 4542, 'basq!iha': 4543, 'basq!ihav': 4544, 'bcm1896wc1': 4545, 'bcm1896wc1n3xx': 4546, 'bcm4284': 4547, 'bcmsfwc1n3xx': 4548, 'big..|': 4549, 'bus8,22,65,61,66,382.': 4550, 'buzzzz!': 4551, 'bx420': 4552, 'bx420-': 4553, 'bx420-i': 4554, 'bx420-ip4-5w': 4555, 'bx420.': 4556, 'bx526,': 4557, 'byåó': 4558, 'byåól': 4559, 'c': 4560, 'c52': 4561, 'c52.': 4562, 'call09050000327': 4563, 'calls1.50ppm': 4564, 'callså£1/mi': 4565, 'callså£1/min': 4566, 'callså£1/minm': 4567, 'cc100p/': 4568, 'cc100p/m': 4569, 'cc100p/mi': 4570, 'cc100p/min': 4571, 'chikku:-):-db-)': 4572, 'chikku:-);-)b-)': 4573, 'chrgd@50p': 4574, 'club4': 4575, 'club4m': 4576, 'club>>': 4577, 'cr01327b': 4578, 'cr01327bt': 4579, 'csh11': 4580, 'cw25wx': 4581, 'd': 4582, 'day..:)\"': 4583, 'days.ì¬': 4584, 'days.ì¬n': 4585, 'e': 4586, 'e.g.23f': 4587, 'e.g.23f.': 4588, 'e.g.23g': 4589, 'e.g.23g.': 4590, 'e14': 4591, 'e=': 4592, 'eh74': 4593, 'eh74r': 4594, 'eh74rr': 4595, 'eshxxxxxxxxxxx': 4596, 'ev': 4597, 'ex': 4598, 'ex-': 4599, 'ex-w': 4600, 'ex-wi': 4601, 'ex-wif': 4602, 'exc': 4603, 'exp': 4604, 'eå£': 4605, 'eå£n': 4606, 'eå£nd': 4607, 'f': 4608, 'f>>>': 4609, 'franyxxxxx': 4610, 'g': 4611, 'g696': 4612, 'g696g': 4613, 'g696ga': 4614, 'gbp1.50/w': 4615, 'gbp4.50/w': 4616, 'gbp5/': 4617, 'gbp5/m': 4618, 'h': 4619, 'half-8': 4620, 'hp20': 4621, 'http://': 4622, 'http://car': 4623, 'http://wap.': 4624, 'http://www.': 4625, 'http://www.bubbl': 4626, 'http://www.urawinn': 4627, 'i': 4628, 'ig11': 4629, 'india:-):': 4630, 'ip4': 4631, 'ip4.': 4632, 'isn\\x89ûª': 4633, 'isn\\x89ûªt': 4634, 'it+b': 4635, 'iwas+m': 4636, 'iwas+ma': 4637, 'iwas+mar': 4638, 'iwas+mari': 4639, 'iwas+marin': 4640, 'i\\x89ûªm': 4641, 'j': 4642, 'j89': 4643, 'j89.': 4644, 'ju': 4645, 'jus': 4646, 'juz': 4647, 'k': 4648, 'k52': 4649, 'k52.': 4650, 'k61': 4651, 'k61.': 4652, 'k718': 4653, 'k718.': 4654, 'k:)': 4655, 'k:)b': 4656, 'k:)bu': 4657, 'k:)but': 4658, 'k:)k': 4659, 'k:)k.': 4660, 'k:)k:)': 4661, 'k:)k:)g': 4662, 'k:)k:)w': 4663, 'k:)k:)wh': 4664, 'k:)k:)wha': 4665, 'k:)k:)what': 4666, 'kl341.': 4667, 'l': 4668, 'la32wu.': 4669, 'ldnw15h': 4670, 'ls15hb': 4671, 'ls278bb': 4672, 'm': 4673, 'm100': 4674, 'm221bp': 4675, 'm221bp.': 4676, 'm227x': 4677, 'm227xy': 4678, 'm227xy.': 4679, 'm26': 4680, 'm263': 4681, 'm263u': 4682, 'm263uz': 4683, 'm263uz.': 4684, 'm39': 4685, 'm39m': 4686, 'm39m51': 4687, 'm6': 4688, 'm95': 4689, 'm95.': 4690, 'matrix3,': 4691, 'max10mi': 4692, 'max10min': 4693, 'max10mins': 4694, 'max6/m': 4695, 'maxå£7.': 4696, 'mini!!!!': 4697, 'mins&100': 4698, 'mins&100t': 4699, 'mins&100tx': 4700, 'mins&100txt': 4701, 'mins&100txt/': 4702, 'mins&100txt/m': 4703, 'mins&100txt/mt': 4704, 'mins&100txt/mth': 4705, 'mix\\\\\"': 4706, 'mk45': 4707, 'mp3': 4708, 'mrng:-)\"': 4709, 'msg+': 4710, 'msg150p': 4711, 'msg@å£1.50': 4712, 'msg@å£1.50r': 4713, 'msg@å£1.50rc': 4714, 'msg@å£1.50rcv': 4715, 'msg@å£1.50rcvd': 4716, 'msgrcvd18+': 4717, 'msgs:d;):': 4718, 'msgs@150p': 4719, 'n': 4720, 'ni8\"': 4721, 'ni8;-)': 4722, 'nr31': 4723, 'nyt:-*': 4724, 'nìâ': 4725, 'o': 4726, 'o2:': 4727, 'of': 4728, 'off': 4729, 'off:-)': 4730, 'ofå£2000': 4731, 'p': 4732, 'park.6ph': 4733, 'ph:08700435505150p': 4734, 'ph:08704050406)': 4735, 'ppm150': 4736, 'ppt150x3+n': 4737, 'px3748': 4738, 'på£3': 4739, 'på£3.': 4740, 'på£3.99': 4741, 'q': 4742, 'q!': 4743, 'q?': 4744, 'qu': 4745, 'r': 4746, 'r836': 4747, 'r836.': 4748, 'rg21': 4749, 'rp176781': 4750, 'rp176781.': 4751, 's': 4752, 's3x': 4753, 's3xy': 4754, 's89': 4755, 's89.': 4756, 's:)8': 4757, 'said:\\\\if': 4758, 'salary..:-);-)': 4759, 'shit....!!\\\\\"': 4760, 'shldxxxx': 4761, 'sk3': 4762, 'sk38x': 4763, 'sk38xh': 4764, 'sk38xh,': 4765, 'sk38xh.': 4766, 'sms-08718727870': 4767, 'sq825,': 4768, 'standing...|': 4769, 'starwars3,': 4770, 'stuff!\"': 4771, 'stuff42m': 4772, 'sudn;-(.': 4773, 'sun0819': 4774, 'sw7': 4775, 'sw73': 4776, 'sw73s': 4777, 'sw73ss': 4778, 't': 4779, 't&c': 4780, 't&c:': 4781, 't&csbcm4235wc1': 4782, 't&csbcm4235wc1n3xx': 4783, 't&csbcm4235wc1n3xx.': 4784, 't91': 4785, 't91.': 4786, 'thanx4': 4787, 'ts&cs': 4788, 'tscs08714740323': 4789, 'tscs087147403231w': 4790, 'tscs087147403231wi': 4791, 'tscs087147403231win': 4792, 'tscs087147403231wina': 4793, 'tscs087147403231winaw': 4794, 'tscs087147403231winawk': 4795, 'tscs087147403231winawk!': 4796, 'tscs087147403231winawk!a': 4797, 'tscs087147403231winawk!ag': 4798, 'txt250.c': 4799, 'txt~j': 4800, 'u': 4801, 'u!xxxx': 4802, 'u4': 4803, 'up+': 4804, 'up+n': 4805, 'up4': 4806, 'u\\x89ûª': 4807, 'u\\x89ûªv': 4808, 'v': 4809, 'valid12hrs': 4810, 'w': 4811, 'w/q': 4812, 'w/qu': 4813, 'w111wx': 4814, 'w14': 4815, 'w14r': 4816, 'w14rg': 4817, 'w1j': 4818, 'w1t1jy': 4819, 'w45wq': 4820, 'wa14': 4821, 'wc1n3xx': 4822, 'win150ppmx3': 4823, 'win150ppmx3a': 4824, 'win150ppmx3ag': 4825, 'wml?id=1b6': 4826, 'wml?id=1b6a': 4827, 'wml?id=1b6a5': 4828, 'wml?id=820554': 4829, 'wml?id=820554a': 4830, 'wml?id=820554ad0': 4831, 'wml?id=820554ad0a': 4832, 'wml?id=820554ad0a1705572711&firs': 4833, 'wrk!\\\\\"\"': 4834, 'www.07781482378': 4835, 'www.07781482378.': 4836, 'www.07781482378.c': 4837, 'www.4-': 4838, 'www.4-t': 4839, 'www.4-tc': 4840, 'www.4-tc.': 4841, 'www.4-tc.b': 4842, 'www.4-tc.bi': 4843, 'www.4-tc.biz': 4844, 'www.80488': 4845, 'www.80488.': 4846, 'www.80488.b': 4847, 'www.80488.bi': 4848, 'www.80488.biz': 4849, 'www.b4u': 4850, 'www.b4ut': 4851, 'www.txt-2-sh': 4852, 'www.txt43.c': 4853, 'www.txt82228.c': 4854, 'www.win-82050.c': 4855, 'x': 4856, 'x2': 4857, 'x29': 4858, 'x29.': 4859, 'x49': 4860, 'x49.': 4861, 'x49.y': 4862, 'x\\\\\"\"': 4863, 'xx': 4864, 'xxx': 4865, 'xxx\\\\\"\"': 4866, 'xxxx': 4867, 'xxxxx': 4868, 'xxxxxx': 4869, 'xxxxxxx': 4870, 'xxxxxxx\\\\\"\"': 4871, 'xxxxxxxx': 4872, 'xxxxxxxxx': 4873, 'xxxxxxxxxx': 4874, 'xxxxxxxxxxx': 4875, 'xxxxxxxxxxxx': 4876, 'xxxxxxxxxxxxx': 4877, 'xxxxxxxxxxxxxx': 4878, 'y': 4879, 'y87': 4880, 'y87.': 4881, 'z': 4882, '|': 4883, '||': 4884, '~': 4885, '\\x89': 4886, '\\x89û': 4887, '\\x89û_': 4888, '\\x89ûï': 4889, '\\x89ûò': 4890, 'å': 4891, 'å£': 4892, 'å£1': 4893, 'å£1,500': 4894, 'å£1.50': 4895, 'å£1.50/m': 4896, 'å£1.50/ms': 4897, 'å£1.50/msg': 4898, 'å£1.50/w': 4899, 'å£1.50/wk': 4900, 'å£1.50/wk.': 4901, 'å£1.50/wk.\"': 4902, 'å£1.50p': 4903, 'å£1.50pm': 4904, 'å£1.50pmm': 4905, 'å£1/': 4906, 'å£1/m': 4907, 'å£1/mi': 4908, 'å£1/min': 4909, 'å£1/minm': 4910, 'å£10': 4911, 'å£10)': 4912, 'å£10,000': 4913, 'å£100': 4914, 'å£100,000': 4915, 'å£1000': 4916, 'å£12': 4917, 'å£125': 4918, 'å£1250': 4919, 'å£14': 4920, 'å£1450': 4921, 'å£15': 4922, 'å£150': 4923, 'å£1500': 4924, 'å£2': 4925, 'å£2,': 4926, 'å£2,0': 4927, 'å£2,00': 4928, 'å£2,000': 4929, 'å£2.50': 4930, 'å£20': 4931, 'å£200': 4932, 'å£2000': 4933, 'å£25': 4934, 'å£250': 4935, 'å£3': 4936, 'å£3.00': 4937, 'å£3/': 4938, 'å£3/w': 4939, 'å£3/wk': 4940, 'å£33': 4941, 'å£33.': 4942, 'å£33.65': 4943, 'å£33:': 4944, 'å£33:5': 4945, 'å£33:50': 4946, 'å£35': 4947, 'å£350': 4948, 'å£350!': 4949, 'å£4': 4950, 'å£4.50': 4951, 'å£4.50.': 4952, 'å£40': 4953, 'å£400': 4954, 'å£48': 4955, 'å£48,': 4956, 'å£5': 4957, 'å£5/': 4958, 'å£5/m': 4959, 'å£50': 4960, 'å£50-å£5': 4961, 'å£50-å£50': 4962, 'å£50-å£500': 4963, 'å£50-å£500.': 4964, 'å£500': 4965, 'å£5000': 4966, 'å£5000.00': 4967, 'å£54': 4968, 'å£54.': 4969, 'å£6': 4970, 'å£60': 4971, 'å£600': 4972, 'å£600.': 4973, 'å£7': 4974, 'å£71': 4975, 'å£71.': 4976, 'å£75': 4977, 'å£75,': 4978, 'å£75,0': 4979, 'å£75,00': 4980, 'å£75,000': 4981, 'å£75,000.': 4982, 'å£750': 4983, 'å£79': 4984, 'å£8': 4985, 'å£80': 4986, 'å£800': 4987, 'å£9': 4988, 'å£90': 4989, 'å£900': 4990, 'å£s': 4991, 'åè': 4992, 'åè1': 4993, 'åè10': 4994, 'åð': 4995, 'åò': 4996, 'åô': 4997, 'åôm': 4998, 'åôr': 4999, 'ì': 5000, 'ì_': 5001, 'ì©': 5002, 'ìï': 5003}\n"
          ]
        }
      ],
      "source": [
        "print(w.vocab_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nqZ82DenXla",
        "outputId": "46c0de24-a1dd-4300-c3b2-1853127ccd74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5004"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(w.vocab_l)\n",
        "\n",
        "vocab_size"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vXgfzz-lnXlb"
      },
      "source": [
        "## Encoding the Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMdWTG1qnXlc",
        "outputId": "f4e0089b-f059-4e15-fd07-6b28790606c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting max text length for padding\n",
        "max_len = df[\"Cl_Text\"].str.len().max()\n",
        "\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CAgqGzvJnXlc"
      },
      "outputs": [],
      "source": [
        "max_len += 1 # due to [SEP] at the end of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DldxCaGYnXld"
      },
      "outputs": [],
      "source": [
        "df[\"En_Text\"] = df[\"Cl_Text\"].apply(lambda x: [w.vocab_d[\"[CLS]\"]] + w.encode(text=x, npad=max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Nn2Mm1PfnXld"
      },
      "outputs": [],
      "source": [
        "max_len += 1 # due to the CLS token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "B5UrqHy5nXle",
        "outputId": "824bfbd6-f93c-43a2-8f95-1afcd22eee8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8af1e4a5-8319-436c-91e0-e8b314d66ad2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cl_Text</th>\n",
              "      <th>En_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "      <td>[0, 4611, 1519, 3, 4801, 1494, 1691, 1331, 139...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "      <td>[0, 4726, 1369, 3, 4668, 1127, 1598, 167, 167,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "      <td>[0, 4608, 1598, 1246, 1246, 3, 4586, 1494, 169...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "      <td>[0, 4801, 3, 4582, 1755, 1494, 3, 4752, 1127, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "      <td>[0, 4720, 1127, 1310, 3, 4628, 3, 4582, 1519, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>[0, 4779, 1310, 1331, 1624, 3, 4628, 1624, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>will ì_ b going to esplanade fr home?</td>\n",
              "      <td>[0, 4811, 1331, 1399, 1399, 3, 5001, 3, 4533, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>pity, * was in mood for that. so...any other s...</td>\n",
              "      <td>[0, 4732, 1331, 1691, 1963, 102, 3, 2176, 3, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>the guy did some bitching but i acted like i'd...</td>\n",
              "      <td>[0, 4779, 1310, 1246, 3, 4611, 1755, 1963, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>rofl. its true to its name</td>\n",
              "      <td>[0, 4746, 1519, 1247, 1399, 167, 3, 4628, 1691...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8af1e4a5-8319-436c-91e0-e8b314d66ad2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8af1e4a5-8319-436c-91e0-e8b314d66ad2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8af1e4a5-8319-436c-91e0-e8b314d66ad2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Label                                               Text  \\\n",
              "0         0  Go until jurong point, crazy.. Available only ...   \n",
              "1         0                      Ok lar... Joking wif u oni...   \n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3         0  U dun say so early hor... U c already then say...   \n",
              "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
              "...     ...                                                ...   \n",
              "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
              "5568      0              Will Ì_ b going to esplanade fr home?   \n",
              "5569      0  Pity, * was in mood for that. So...any other s...   \n",
              "5570      0  The guy did some bitching but I acted like i'd...   \n",
              "5571      0                         Rofl. Its true to its name   \n",
              "\n",
              "                                                Cl_Text  \\\n",
              "0     go until jurong point, crazy.. available only ...   \n",
              "1                         ok lar... joking wif u oni...   \n",
              "2     free entry in 2 a wkly comp to win fa cup fina...   \n",
              "3     u dun say so early hor... u c already then say...   \n",
              "4     nah i don't think he goes to usf, he lives aro...   \n",
              "...                                                 ...   \n",
              "5567  this is the 2nd time we have tried 2 contact u...   \n",
              "5568              will ì_ b going to esplanade fr home?   \n",
              "5569  pity, * was in mood for that. so...any other s...   \n",
              "5570  the guy did some bitching but i acted like i'd...   \n",
              "5571                         rofl. its true to its name   \n",
              "\n",
              "                                                En_Text  \n",
              "0     [0, 4611, 1519, 3, 4801, 1494, 1691, 1331, 139...  \n",
              "1     [0, 4726, 1369, 3, 4668, 1127, 1598, 167, 167,...  \n",
              "2     [0, 4608, 1598, 1246, 1246, 3, 4586, 1494, 169...  \n",
              "3     [0, 4801, 3, 4582, 1755, 1494, 3, 4752, 1127, ...  \n",
              "4     [0, 4720, 1127, 1310, 3, 4628, 3, 4582, 1519, ...  \n",
              "...                                                 ...  \n",
              "5567  [0, 4779, 1310, 1331, 1624, 3, 4628, 1624, 3, ...  \n",
              "5568  [0, 4811, 1331, 1399, 1399, 3, 5001, 3, 4533, ...  \n",
              "5569  [0, 4732, 1331, 1691, 1963, 102, 3, 2176, 3, 4...  \n",
              "5570  [0, 4779, 1310, 1246, 3, 4611, 1755, 1963, 3, ...  \n",
              "5571  [0, 4746, 1519, 1247, 1399, 167, 3, 4628, 1691...  \n",
              "\n",
              "[5572 rows x 4 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXFyuE54nXle",
        "outputId": "d51c4e89-6871-4869-cb41-0d491ed42f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "912\n",
            "912\n"
          ]
        }
      ],
      "source": [
        "print(df[\"En_Text\"].str.len().max())\n",
        "print(max_len)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0DMqxfzSnXlf"
      },
      "source": [
        "## Converting DataFrame to Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "egw8W7dbnXlg"
      },
      "outputs": [],
      "source": [
        "class FinancialNewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.samples = [(dataframe[\"En_Text\"][i], dataframe[\"Label\"][i]) for i in range(len(dataframe))]\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "        self.idx_to_classes = {i: c for i, c in enumerate(self.classes)}\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if isinstance(index, slice):\n",
        "            return [(torch.tensor(sample[0], dtype=torch.long), sample[1]) for sample in self.samples[index]] # List (Tuple (Tensor, Int) )\n",
        "        return (torch.tensor(self.samples[index][0], dtype=torch.long), self.samples[index][1])               # Tuple (Tensor, Int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZNlDHzvnXlg",
        "outputId": "49ca2dc7-0f35-4f14-e3bd-82c4dc37a231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5572\n",
            "[(tensor([   0, 4726, 1369,    3, 4668, 1127, 1598,  167,  167,  167,    3, 4642,\n",
            "        1519, 1369, 1331, 1494, 1275,    3, 4811, 1331, 1247,    3, 4801,    3,\n",
            "        4726, 1494, 1331,  167,  167,  167,    3,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2]), 0), (tensor([   0, 4608, 1598, 1246, 1246,    3, 4586, 1494, 1691, 1598, 1963,    3,\n",
            "        4628, 1494,    3, 3749,    3, 4517,    3, 4811, 1369, 1399, 1963,    3,\n",
            "        4560, 1519, 1447, 1520,    3, 4779, 1519,    3, 4811, 1331, 1494,    3,\n",
            "        4608, 1127,    3, 4560, 1755, 1520,    3, 4608, 1331, 1494, 1127, 1399,\n",
            "           3, 4779, 1369, 1691, 1624,    3, 3783, 1624, 1691,    3, 4673, 1127,\n",
            "        1963,    3, 3769,    3, 4779, 1246, 1841, 1691,    3, 4608, 1127,    3,\n",
            "        4779, 1519,    3, 4346,    3, 4779, 1519,    3, 4746, 1246, 1192, 1246,\n",
            "        1331, 1783, 1246,    3, 4586, 1494, 1691, 1598, 1963,    3, 4745, 1246,\n",
            "        1624, 1691, 1331, 1519, 1494,   65, 1624, 1691, 1213,    3, 4779, 1841,\n",
            "        1691,    3, 4746, 1127, 1691, 1246,   78, 1694,   64, 1624,    3, 4517,\n",
            "        1520, 1520, 1399, 1963,    3, 2601, 1519, 1783, 1246, 1601,    3,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2]), 1)]\n",
            "(tensor([   0, 4619, 1127, 1213,    3, 4879, 1519, 1755, 1598,    3, 4673, 1519,\n",
            "        1173, 1331, 1399, 1246,    3, 3640,    3, 4673, 1519, 1494, 1691, 1310,\n",
            "        1624,    3, 4726, 1598,    3, 4673, 1519, 1598, 1246, 1089,    3, 4801,\n",
            "           3, 4746,    3, 4586, 1494, 1691, 1331, 1691, 1399, 1246, 1213,    3,\n",
            "        4779, 1519,    3, 4801, 1520, 1213, 1127, 1691, 1246,    3, 4779, 1519,\n",
            "           3, 4779, 1310, 1246,    3, 4668, 1127, 1691, 1246, 1624, 1691,    3,\n",
            "        4560, 1519, 1399, 1519, 1755, 1598,    3, 4673, 1519, 1173, 1331, 1399,\n",
            "        1246, 1624,    3, 4811, 1331, 1691, 1310,    3, 4560, 1127, 1447, 1246,\n",
            "        1598, 1127,    3, 4608, 1519, 1598,    3, 4608, 1598, 1246, 1246,   14,\n",
            "           3, 4560, 1127, 1399, 1399,    3, 4779, 1310, 1246,    3, 4673, 1519,\n",
            "        1173, 1331, 1399, 1246,    3, 4801, 1520, 1213, 1127, 1691, 1246,    3,\n",
            "        4560, 1519,    3, 4608, 1598, 1246, 1246,    3, 4726, 1494,    3, 2539,\n",
            "           3,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2]), 1)\n"
          ]
        }
      ],
      "source": [
        "ds = FinancialNewsDataset(df, [\"ham\", \"spam\"])\n",
        "\n",
        "print(len(ds))\n",
        "print(ds[1:3])\n",
        "print(ds[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7B9ZTLnXlg",
        "outputId": "f5dac302-d3be-4323-ffed-7b9901bbe625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ham': 0, 'spam': 1}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes = ds.classes\n",
        "class_to_idx = ds.class_to_idx\n",
        "idx_to_classes = ds.idx_to_classes\n",
        "\n",
        "class_to_idx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "35eZw080nXlh"
      },
      "source": [
        "## Splitting the Dataset into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZrrm0IrnXlh",
        "outputId": "bee70abc-cf46-4a35-ec83-d234fdcecda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5293 279\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(ds) * 0.95)\n",
        " \n",
        "train_ds = ds[:train_size]\n",
        "test_ds = ds[train_size:]\n",
        "\n",
        "print(len(train_ds), len(test_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3pX1L_klnXlh"
      },
      "source": [
        "## Creating DataLoader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tOccY9panXlh"
      },
      "outputs": [],
      "source": [
        "class Loader:\n",
        "    def __init__(self, ds, batch_size, shuffle):\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self._dsx = [s[0].tolist() for s in ds] # contains the x-values (inputs) of the dataset | `tolist()` converts the x-tensor into Python List | List (List (Int) )\n",
        "        self._dsy = [s[1] for s in ds]          # contains the y-values (targets) of the dataset | List (Int)\n",
        "\n",
        "        if shuffle:\n",
        "            self._temp_dsx = self._dsx.copy() \n",
        "            self._temp_dsy = self._dsy.copy()\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            # Iterating over the number of batches that the dataset is going to bet split\n",
        "            for _ in range(len(self._dsx) // self.batch_size):\n",
        "                \n",
        "                # This random index gives the index of the first sample for the batch\n",
        "                ridx = randint(0, len(self._temp_dsx) - self.batch_size)\n",
        "\n",
        "                yield (torch.tensor(self._temp_dsx[ridx: ridx + self.batch_size], dtype=torch.long), torch.tensor(self._temp_dsy[ridx: ridx + self.batch_size], dtype=torch.long))\n",
        "\n",
        "                # Removing the already `yield`ed batch from the dataset\n",
        "                self._temp_dsx = self._temp_dsx[:ridx] + self._temp_dsx[ridx + self.batch_size:]\n",
        "                self._temp_dsy = self._temp_dsy[:ridx] + self._temp_dsy[ridx + self.batch_size:]\n",
        "\n",
        "            # Returning the last batch, which is not going to contain `batch_size` samples\n",
        "            if len(self._temp_dsx) > 0:\n",
        "                yield (torch.tensor(self._temp_dsx, dtype=torch.long), torch.tensor(self._temp_dsy, dtype=torch.long))\n",
        "\n",
        "            # If we try to iterate again over the loader without those two lines, no samples are going to be returned\n",
        "            self._temp_dsx = self._dsx.copy()\n",
        "            self._temp_dsy = self._dsy.copy()\n",
        "\n",
        "        else:\n",
        "            j = 0\n",
        "            for _ in range(ceil(len(self._dsx) / self.batch_size)):\n",
        "                yield (torch.tensor(self._dsx[j: j + self.batch_size], dtype=torch.long), torch.tensor(self._dsy[j: j + self.batch_size], dtype=torch.long))\n",
        "                j += self.batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return ceil(len(self._dsx) / self.batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "75D8OJKinXli"
      },
      "source": [
        "## Creating the Cross Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NAcmygMinXli"
      },
      "outputs": [],
      "source": [
        "def cross_validation(ds, valid_prop, batch_size):\n",
        "    valid_size = int(len(ds) * valid_prop)\n",
        "    ridx = randint(0, len(ds) - valid_size)\n",
        "\n",
        "    return (Loader(ds[ridx: ridx + valid_size], batch_size=batch_size, shuffle=False), Loader(ds[:ridx] + ds[ridx + valid_size:], batch_size=batch_size, shuffle=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt6w5ksvnXlj"
      },
      "source": [
        "## Creating Model's Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Vqs3Y04unXlj"
      },
      "outputs": [],
      "source": [
        "class ModelUtils(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "\n",
        "    def __training_step(self, train_dl, opt, device):\n",
        "        losses = torch.zeros(len(train_dl), device=device)\n",
        "        for i, (x_train, y_train) in enumerate(train_dl):\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "\n",
        "            _, loss = self(x_train, y_train)\n",
        "            losses[i] = loss.item()\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        return losses.mean().item()\n",
        "\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def __validation_step(self, valid_dl, device):\n",
        "        self.eval()\n",
        "        losses = torch.zeros(len(valid_dl), device=device)\n",
        "        for i, (x_train, y_train) in enumerate(valid_dl):\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "\n",
        "            _, loss = self(x_train, y_train)\n",
        "            losses[i] = loss.item()\n",
        "\n",
        "        self.train()\n",
        "        return losses.mean().item()\n",
        "\n",
        "\n",
        "    def fit(self, epochs, train_ds, opt):\n",
        "        start_time = timer()\n",
        "        device = next(self.parameters()).device\n",
        "        train_losses, valid_losses = [], []\n",
        "\n",
        "        t = tqdm(range(1, epochs + 1), desc=\"Training Model: \")\n",
        "        t.set_postfix({\"train_loss\": \"inf\", \"valid_loss\": \"inf\"})\n",
        "        for _ in t:\n",
        "            valid_dl, train_dl = cross_validation(train_ds, valid_prop=0.2, batch_size=32)\n",
        "        \n",
        "            train_loss = self.__training_step(train_dl, opt, device)\n",
        "            valid_loss = self.__validation_step(valid_dl, device)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "            t.set_postfix({\"train_loss\": train_loss, \"valid_loss\": valid_loss})\n",
        "            t.refresh()\n",
        "\n",
        "        return {\"model_train_loss\": train_losses,\n",
        "            \"model_valid_loss\": valid_losses,\n",
        "            \"model_name\": self.__class__.__name__,\n",
        "            \"model_optimizer\": opt.__class__.__name__,\n",
        "            \"model_device\": device.type,\n",
        "            \"model_epochs\": epochs,\n",
        "            \"model_time\": timer() - start_time}\n",
        "\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def evaluate(self, dl):\n",
        "        self.eval()\n",
        "\n",
        "        device = next(self.parameters()).device\n",
        "        metric_collection = MetricCollection([\n",
        "            Accuracy(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            Precision(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            Recall(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            F1Score(task=\"multiclass\", num_classes=2, average=\"macro\")\n",
        "        ]).to(device)\n",
        "        losses = torch.zeros(len(dl))\n",
        "\n",
        "        for i, (xb, yb) in enumerate(dl):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            logits, loss = self(xb, yb)\n",
        "            preds = F.softmax(logits, dim=-1)\n",
        "\n",
        "            metric_collection.update(preds[:, 0, :], yb)\n",
        "            losses[i] = loss.item()\n",
        "        \n",
        "        res = metric_collection.compute()\n",
        "        \n",
        "        self.train()\n",
        "        return losses.mean().item(), res[\"MulticlassAccuracy\"].item(), res[\"MulticlassPrecision\"].item(), res[\"MulticlassRecall\"].item(), res[\"MulticlassF1Score\"].item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kX3CTEvpnXlk"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IAYpTIOSnXlk"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, C = x.shape\n",
        "\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * (C**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, embed_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([SelfAttention(embed_size, head_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(embed_size, embed_size, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Contatenate the outputs of each Masked Self-Attention\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, EMBED_SIZE)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, scale_embeds, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, scale_embeds * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(scale_embeds * embed_size, embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) # (B, T, EMBED_SIZE)\n",
        "    \n",
        "class Block(nn.Module): # combining Masked Multi-Head Attention and one Feed-Forward layer\n",
        "    def __init__(self, embed_size, scale_embeds, num_heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = embed_size // num_heads # because the result of the Masked Multi-Head layer we want to have shape: (B, T, EMBED_SIZE)\n",
        "        self.multi_att_m = MultiHeadAttention(num_heads, embed_size, head_size, dropout)\n",
        "        self.ffwd = FeedForward(embed_size, scale_embeds, dropout)\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = x + self.multi_att_m(self.ln1(x)) # (B, T, EMBED_SIZE)\n",
        "        x = x + self.ffwd(self.ln2(x))        # (B, T, EMBED_SIZE)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class TransformerEncoder(ModelUtils):\n",
        "    def __init__(self, embed_size, num_layers, scale_embeds, num_heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # Self-Attention doesn't take into consideration the position of tokens when computing the attetnion matrix, so we have to\n",
        "        self.position_embedding_table = nn.Embedding(max_len, embed_size)\n",
        "        self.block = nn.Sequential(*[Block(embed_size, scale_embeds, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.linear_head = nn.Linear(embed_size, 2)        \n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        token_embeddings = self.embedding_table(idx)                                              # (B, T, EMBED_SIZE)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(max_len, device=device)) # (T, EMBED_SIZE)\n",
        "\n",
        "        x = token_embeddings + position_embeddings # (B, T, EMBED_SIZE)\n",
        "\n",
        "        x = self.block(x) # (B, T, EMBED_SIZE)\n",
        "        x = self.ln_f(x)  # (B, T, EMBED_SIZE)\n",
        "\n",
        "        logits = self.linear_head(x) # (B, T, 2)\n",
        "\n",
        "        # Condition to seperate training and generating phase\n",
        "        loss = F.cross_entropy(logits[:, 0, :], targets) if targets is not None else None\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecZffMumnXlk",
        "outputId": "7be6de09-a72d-4304-b324-9372102afa4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable\n",
              "=================================================================================================================================================\n",
              "TransformerEncoder                            [1, 912]                  [1, 912, 2]               --                        True\n",
              "├─Embedding: 1-1                              [1, 912]                  [1, 912, 100]             500,400                   True\n",
              "├─Embedding: 1-2                              [912]                     [912, 100]                91,200                    True\n",
              "├─Sequential: 1-3                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    └─Block: 2-1                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    │    └─LayerNorm: 3-1                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─MultiHeadAttention: 3-2           [1, 912, 100]             [1, 912, 100]             40,100                    True\n",
              "│    │    └─LayerNorm: 3-3                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─FeedForward: 3-4                  [1, 912, 100]             [1, 912, 100]             20,200                    True\n",
              "│    └─Block: 2-2                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    │    └─LayerNorm: 3-5                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─MultiHeadAttention: 3-6           [1, 912, 100]             [1, 912, 100]             40,100                    True\n",
              "│    │    └─LayerNorm: 3-7                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─FeedForward: 3-8                  [1, 912, 100]             [1, 912, 100]             20,200                    True\n",
              "├─LayerNorm: 1-4                              [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "├─Linear: 1-5                                 [1, 912, 100]             [1, 912, 2]               202                       True\n",
              "=================================================================================================================================================\n",
              "Total params: 713,402\n",
              "Trainable params: 713,402\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 83.80\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 13.88\n",
              "Params size (MB): 2.85\n",
              "Estimated Total Size (MB): 16.74\n",
              "================================================================================================================================================="
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EMBED_SIZE = 100\n",
        "NUM_LAYERS = 2\n",
        "SCALE_EMBEDS = 1\n",
        "NUM_HEADS = 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = TransformerEncoder(EMBED_SIZE, NUM_LAYERS, SCALE_EMBEDS, NUM_HEADS, DROPOUT).to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(1, max_len),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        dtypes=[torch.int64])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWYIOlpnXll"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VyjVEbcnXll",
        "outputId": "cc975ade-745b-4341-9629-1a78aebea2f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Model: 100%|██████████| 5/5 [00:54<00:00, 10.98s/it, train_loss=0.0964, valid_loss=0.0931]\n"
          ]
        }
      ],
      "source": [
        "opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "res = model.fit(EPOCHS, train_ds, opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmMBMs9mnXll",
        "outputId": "973af522-1c75-495e-faef-783c4298f8dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.11071637272834778, 0.84375, 0.9805447459220886, 0.84375, 0.8974868059158325)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(Loader(test_ds, batch_size=32, shuffle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNe7QoxoqEJv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
